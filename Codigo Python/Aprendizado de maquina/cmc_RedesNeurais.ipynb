{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cmc-RedesNeurais",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujQYDhOVpGhd"
      },
      "source": [
        "Brendo William de Souza Faria / 20114290065\n",
        "Daniel Alves Gonçalves Pires da Silva / 20114290133"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y-mHL4_diOab",
        "outputId": "af9632a7-a7b1-44cb-88de-83c4fba4efad"
      },
      "source": [
        "%reset -f\n",
        "import pickle\n",
        "with open('cmc.pkl', 'rb') as f:  \n",
        "  X_cmc_treinamento, y_cmc_treinamento, X_cmc_teste, y_cmc_teste = pickle.load(f)\n",
        "from sklearn.neural_network import MLPClassifier # Neurônios = (3+1)/2 = 2\n",
        "print(X_cmc_treinamento.shape)\n",
        "rede_neural_cmc = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100, #a cara 1500 interações ele para / taxa de aprendizagem tol\n",
        "                                   solver = 'adam', activation = 'relu',\n",
        "                                   hidden_layer_sizes = (2,2)) #aqui construímos a divisão das redes neurais, construindo a Deep Learning de 2 pra 2 \n",
        "rede_neural_cmc.fit(X_cmc_treinamento, y_cmc_treinamento)\n",
        "previsoes = rede_neural_cmc.predict(X_cmc_teste)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(accuracy_score(y_cmc_teste, previsoes))\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm = ConfusionMatrix(rede_neural_cmc)\n",
        "cm.fit(X_cmc_treinamento, y_cmc_treinamento)\n",
        "cm.score(X_cmc_teste, y_cmc_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1104, 9)\n",
            "Iteration 1, loss = 1.29106435\n",
            "Iteration 2, loss = 1.27388235\n",
            "Iteration 3, loss = 1.25889738\n",
            "Iteration 4, loss = 1.24407764\n",
            "Iteration 5, loss = 1.23082590\n",
            "Iteration 6, loss = 1.21766244\n",
            "Iteration 7, loss = 1.20660705\n",
            "Iteration 8, loss = 1.19622133\n",
            "Iteration 9, loss = 1.18633601\n",
            "Iteration 10, loss = 1.17739147\n",
            "Iteration 11, loss = 1.16918820\n",
            "Iteration 12, loss = 1.16178049\n",
            "Iteration 13, loss = 1.15495288\n",
            "Iteration 14, loss = 1.14826555\n",
            "Iteration 15, loss = 1.14249835\n",
            "Iteration 16, loss = 1.13706841\n",
            "Iteration 17, loss = 1.13208564\n",
            "Iteration 18, loss = 1.12723762\n",
            "Iteration 19, loss = 1.12248704\n",
            "Iteration 20, loss = 1.11846531\n",
            "Iteration 21, loss = 1.11460845\n",
            "Iteration 22, loss = 1.11090464\n",
            "Iteration 23, loss = 1.10765628\n",
            "Iteration 24, loss = 1.10472716\n",
            "Iteration 25, loss = 1.10195427\n",
            "Iteration 26, loss = 1.09932766\n",
            "Iteration 27, loss = 1.09694896\n",
            "Iteration 28, loss = 1.09470968\n",
            "Iteration 29, loss = 1.09272866\n",
            "Iteration 30, loss = 1.09072689\n",
            "Iteration 31, loss = 1.08892030\n",
            "Iteration 32, loss = 1.08730650\n",
            "Iteration 33, loss = 1.08572402\n",
            "Iteration 34, loss = 1.08437117\n",
            "Iteration 35, loss = 1.08293985\n",
            "Iteration 36, loss = 1.08158153\n",
            "Iteration 37, loss = 1.08049681\n",
            "Iteration 38, loss = 1.07935193\n",
            "Iteration 39, loss = 1.07820903\n",
            "Iteration 40, loss = 1.07712230\n",
            "Iteration 41, loss = 1.07596864\n",
            "Iteration 42, loss = 1.07504174\n",
            "Iteration 43, loss = 1.07402544\n",
            "Iteration 44, loss = 1.07311957\n",
            "Iteration 45, loss = 1.07211998\n",
            "Iteration 46, loss = 1.07133120\n",
            "Iteration 47, loss = 1.07056016\n",
            "Iteration 48, loss = 1.06976004\n",
            "Iteration 49, loss = 1.06906257\n",
            "Iteration 50, loss = 1.06831855\n",
            "Iteration 51, loss = 1.06776651\n",
            "Iteration 52, loss = 1.06717806\n",
            "Iteration 53, loss = 1.06659977\n",
            "Iteration 54, loss = 1.06615295\n",
            "Iteration 55, loss = 1.06561775\n",
            "Iteration 56, loss = 1.06517337\n",
            "Iteration 57, loss = 1.06475360\n",
            "Iteration 58, loss = 1.06430832\n",
            "Iteration 59, loss = 1.06392583\n",
            "Iteration 60, loss = 1.06350059\n",
            "Iteration 61, loss = 1.06308540\n",
            "Iteration 62, loss = 1.06272003\n",
            "Iteration 63, loss = 1.06235279\n",
            "Iteration 64, loss = 1.06197511\n",
            "Iteration 65, loss = 1.06161436\n",
            "Iteration 66, loss = 1.06128518\n",
            "Iteration 67, loss = 1.06095128\n",
            "Iteration 68, loss = 1.06064604\n",
            "Iteration 69, loss = 1.06033512\n",
            "Iteration 70, loss = 1.06002573\n",
            "Iteration 71, loss = 1.05978639\n",
            "Iteration 72, loss = 1.05952985\n",
            "Iteration 73, loss = 1.05923833\n",
            "Iteration 74, loss = 1.05902093\n",
            "Iteration 75, loss = 1.05872510\n",
            "Iteration 76, loss = 1.05849986\n",
            "Iteration 77, loss = 1.05827768\n",
            "Iteration 78, loss = 1.05812711\n",
            "Iteration 79, loss = 1.05781343\n",
            "Iteration 80, loss = 1.05757624\n",
            "Iteration 81, loss = 1.05738262\n",
            "Iteration 82, loss = 1.05718837\n",
            "Iteration 83, loss = 1.05696004\n",
            "Iteration 84, loss = 1.05673895\n",
            "Iteration 85, loss = 1.05656873\n",
            "Iteration 86, loss = 1.05632371\n",
            "Iteration 87, loss = 1.05612722\n",
            "Iteration 88, loss = 1.05594733\n",
            "Iteration 89, loss = 1.05569168\n",
            "Iteration 90, loss = 1.05547024\n",
            "Iteration 91, loss = 1.05529123\n",
            "Iteration 92, loss = 1.05505163\n",
            "Iteration 93, loss = 1.05480194\n",
            "Iteration 94, loss = 1.05459152\n",
            "Iteration 95, loss = 1.05435280\n",
            "Iteration 96, loss = 1.05412045\n",
            "Iteration 97, loss = 1.05388450\n",
            "Iteration 98, loss = 1.05368662\n",
            "Iteration 99, loss = 1.05341380\n",
            "Iteration 100, loss = 1.05319049\n",
            "Iteration 101, loss = 1.05296204\n",
            "Iteration 102, loss = 1.05270412\n",
            "Iteration 103, loss = 1.05244276\n",
            "Iteration 104, loss = 1.05225221\n",
            "Iteration 105, loss = 1.05203491\n",
            "Iteration 106, loss = 1.05178155\n",
            "Iteration 107, loss = 1.05151189\n",
            "Iteration 108, loss = 1.05125805\n",
            "Iteration 109, loss = 1.05100317\n",
            "Iteration 110, loss = 1.05083062\n",
            "Iteration 111, loss = 1.05051191\n",
            "Iteration 112, loss = 1.05022253\n",
            "Iteration 113, loss = 1.04993588\n",
            "Iteration 114, loss = 1.04965579\n",
            "Iteration 115, loss = 1.04938893\n",
            "Iteration 116, loss = 1.04908277\n",
            "Iteration 117, loss = 1.04877945\n",
            "Iteration 118, loss = 1.04846035\n",
            "Iteration 119, loss = 1.04813840\n",
            "Iteration 120, loss = 1.04783075\n",
            "Iteration 121, loss = 1.04742468\n",
            "Iteration 122, loss = 1.04707800\n",
            "Iteration 123, loss = 1.04672591\n",
            "Iteration 124, loss = 1.04633045\n",
            "Iteration 125, loss = 1.04598040\n",
            "Iteration 126, loss = 1.04565712\n",
            "Iteration 127, loss = 1.04533777\n",
            "Iteration 128, loss = 1.04497975\n",
            "Iteration 129, loss = 1.04469438\n",
            "Iteration 130, loss = 1.04434151\n",
            "Iteration 131, loss = 1.04402654\n",
            "Iteration 132, loss = 1.04363014\n",
            "Iteration 133, loss = 1.04329103\n",
            "Iteration 134, loss = 1.04294706\n",
            "Iteration 135, loss = 1.04262993\n",
            "Iteration 136, loss = 1.04230752\n",
            "Iteration 137, loss = 1.04200606\n",
            "Iteration 138, loss = 1.04173637\n",
            "Iteration 139, loss = 1.04139916\n",
            "Iteration 140, loss = 1.04110122\n",
            "Iteration 141, loss = 1.04074772\n",
            "Iteration 142, loss = 1.04038514\n",
            "Iteration 143, loss = 1.04001787\n",
            "Iteration 144, loss = 1.03967921\n",
            "Iteration 145, loss = 1.03929164\n",
            "Iteration 146, loss = 1.03893055\n",
            "Iteration 147, loss = 1.03852347\n",
            "Iteration 148, loss = 1.03819954\n",
            "Iteration 149, loss = 1.03773213\n",
            "Iteration 150, loss = 1.03734113\n",
            "Iteration 151, loss = 1.03686714\n",
            "Iteration 152, loss = 1.03640527\n",
            "Iteration 153, loss = 1.03585579\n",
            "Iteration 154, loss = 1.03531307\n",
            "Iteration 155, loss = 1.03464641\n",
            "Iteration 156, loss = 1.03406656\n",
            "Iteration 157, loss = 1.03343165\n",
            "Iteration 158, loss = 1.03276411\n",
            "Iteration 159, loss = 1.03212208\n",
            "Iteration 160, loss = 1.03156989\n",
            "Iteration 161, loss = 1.03088238\n",
            "Iteration 162, loss = 1.03026500\n",
            "Iteration 163, loss = 1.02964485\n",
            "Iteration 164, loss = 1.02900989\n",
            "Iteration 165, loss = 1.02838855\n",
            "Iteration 166, loss = 1.02793296\n",
            "Iteration 167, loss = 1.02740804\n",
            "Iteration 168, loss = 1.02693150\n",
            "Iteration 169, loss = 1.02650564\n",
            "Iteration 170, loss = 1.02606078\n",
            "Iteration 171, loss = 1.02559056\n",
            "Iteration 172, loss = 1.02516776\n",
            "Iteration 173, loss = 1.02478585\n",
            "Iteration 174, loss = 1.02433964\n",
            "Iteration 175, loss = 1.02396938\n",
            "Iteration 176, loss = 1.02361946\n",
            "Iteration 177, loss = 1.02317614\n",
            "Iteration 178, loss = 1.02276730\n",
            "Iteration 179, loss = 1.02236070\n",
            "Iteration 180, loss = 1.02194626\n",
            "Iteration 181, loss = 1.02157880\n",
            "Iteration 182, loss = 1.02119709\n",
            "Iteration 183, loss = 1.02090857\n",
            "Iteration 184, loss = 1.02054556\n",
            "Iteration 185, loss = 1.02021607\n",
            "Iteration 186, loss = 1.01995447\n",
            "Iteration 187, loss = 1.01959860\n",
            "Iteration 188, loss = 1.01928972\n",
            "Iteration 189, loss = 1.01901137\n",
            "Iteration 190, loss = 1.01867447\n",
            "Iteration 191, loss = 1.01837263\n",
            "Iteration 192, loss = 1.01810823\n",
            "Iteration 193, loss = 1.01783890\n",
            "Iteration 194, loss = 1.01756549\n",
            "Iteration 195, loss = 1.01735638\n",
            "Iteration 196, loss = 1.01708813\n",
            "Iteration 197, loss = 1.01679578\n",
            "Iteration 198, loss = 1.01659122\n",
            "Iteration 199, loss = 1.01630162\n",
            "Iteration 200, loss = 1.01607210\n",
            "Iteration 201, loss = 1.01586695\n",
            "Iteration 202, loss = 1.01558969\n",
            "Iteration 203, loss = 1.01537937\n",
            "Iteration 204, loss = 1.01517735\n",
            "Iteration 205, loss = 1.01490741\n",
            "Iteration 206, loss = 1.01471357\n",
            "Iteration 207, loss = 1.01453479\n",
            "Iteration 208, loss = 1.01428519\n",
            "Iteration 209, loss = 1.01411585\n",
            "Iteration 210, loss = 1.01388522\n",
            "Iteration 211, loss = 1.01366131\n",
            "Iteration 212, loss = 1.01347693\n",
            "Iteration 213, loss = 1.01332462\n",
            "Iteration 214, loss = 1.01308030\n",
            "Iteration 215, loss = 1.01288791\n",
            "Iteration 216, loss = 1.01272094\n",
            "Iteration 217, loss = 1.01252845\n",
            "Iteration 218, loss = 1.01230923\n",
            "Iteration 219, loss = 1.01213468\n",
            "Iteration 220, loss = 1.01197911\n",
            "Iteration 221, loss = 1.01170573\n",
            "Iteration 222, loss = 1.01157716\n",
            "Iteration 223, loss = 1.01140160\n",
            "Iteration 224, loss = 1.01121223\n",
            "Iteration 225, loss = 1.01099114\n",
            "Iteration 226, loss = 1.01080395\n",
            "Iteration 227, loss = 1.01065550\n",
            "Iteration 228, loss = 1.01048248\n",
            "Iteration 229, loss = 1.01034275\n",
            "Iteration 230, loss = 1.01017599\n",
            "Iteration 231, loss = 1.01004429\n",
            "Iteration 232, loss = 1.00981964\n",
            "Iteration 233, loss = 1.00968996\n",
            "Iteration 234, loss = 1.00944996\n",
            "Iteration 235, loss = 1.00938368\n",
            "Iteration 236, loss = 1.00911415\n",
            "Iteration 237, loss = 1.00894911\n",
            "Iteration 238, loss = 1.00873625\n",
            "Iteration 239, loss = 1.00860439\n",
            "Iteration 240, loss = 1.00838437\n",
            "Iteration 241, loss = 1.00824518\n",
            "Iteration 242, loss = 1.00810254\n",
            "Iteration 243, loss = 1.00791721\n",
            "Iteration 244, loss = 1.00780161\n",
            "Iteration 245, loss = 1.00769876\n",
            "Iteration 246, loss = 1.00759387\n",
            "Iteration 247, loss = 1.00745766\n",
            "Iteration 248, loss = 1.00723901\n",
            "Iteration 249, loss = 1.00708974\n",
            "Iteration 250, loss = 1.00692151\n",
            "Iteration 251, loss = 1.00677031\n",
            "Iteration 252, loss = 1.00658664\n",
            "Iteration 253, loss = 1.00652677\n",
            "Iteration 254, loss = 1.00636055\n",
            "Iteration 255, loss = 1.00624793\n",
            "Iteration 256, loss = 1.00609759\n",
            "Iteration 257, loss = 1.00590846\n",
            "Iteration 258, loss = 1.00587374\n",
            "Iteration 259, loss = 1.00569127\n",
            "Iteration 260, loss = 1.00557029\n",
            "Iteration 261, loss = 1.00537487\n",
            "Iteration 262, loss = 1.00532224\n",
            "Iteration 263, loss = 1.00512961\n",
            "Iteration 264, loss = 1.00502994\n",
            "Iteration 265, loss = 1.00489231\n",
            "Iteration 266, loss = 1.00479769\n",
            "Iteration 267, loss = 1.00465524\n",
            "Iteration 268, loss = 1.00450219\n",
            "Iteration 269, loss = 1.00437849\n",
            "Iteration 270, loss = 1.00423762\n",
            "Iteration 271, loss = 1.00407289\n",
            "Iteration 272, loss = 1.00398404\n",
            "Iteration 273, loss = 1.00384054\n",
            "Iteration 274, loss = 1.00365198\n",
            "Iteration 275, loss = 1.00355907\n",
            "Iteration 276, loss = 1.00347485\n",
            "Iteration 277, loss = 1.00329050\n",
            "Iteration 278, loss = 1.00318300\n",
            "Iteration 279, loss = 1.00306494\n",
            "Iteration 280, loss = 1.00295170\n",
            "Iteration 281, loss = 1.00283334\n",
            "Iteration 282, loss = 1.00279421\n",
            "Iteration 283, loss = 1.00255535\n",
            "Iteration 284, loss = 1.00242903\n",
            "Iteration 285, loss = 1.00231452\n",
            "Iteration 286, loss = 1.00228335\n",
            "Iteration 287, loss = 1.00208740\n",
            "Iteration 288, loss = 1.00199622\n",
            "Iteration 289, loss = 1.00187272\n",
            "Iteration 290, loss = 1.00177823\n",
            "Iteration 291, loss = 1.00166267\n",
            "Iteration 292, loss = 1.00151644\n",
            "Iteration 293, loss = 1.00142047\n",
            "Iteration 294, loss = 1.00127428\n",
            "Iteration 295, loss = 1.00117227\n",
            "Iteration 296, loss = 1.00105645\n",
            "Iteration 297, loss = 1.00090743\n",
            "Iteration 298, loss = 1.00078875\n",
            "Iteration 299, loss = 1.00071382\n",
            "Iteration 300, loss = 1.00062864\n",
            "Iteration 301, loss = 1.00053091\n",
            "Iteration 302, loss = 1.00042651\n",
            "Iteration 303, loss = 1.00029003\n",
            "Iteration 304, loss = 1.00015018\n",
            "Iteration 305, loss = 1.00007765\n",
            "Iteration 306, loss = 0.99995680\n",
            "Iteration 307, loss = 0.99984753\n",
            "Iteration 308, loss = 0.99970629\n",
            "Iteration 309, loss = 0.99965763\n",
            "Iteration 310, loss = 0.99952302\n",
            "Iteration 311, loss = 0.99942179\n",
            "Iteration 312, loss = 0.99932406\n",
            "Iteration 313, loss = 0.99920852\n",
            "Iteration 314, loss = 0.99913688\n",
            "Iteration 315, loss = 0.99899504\n",
            "Iteration 316, loss = 0.99884718\n",
            "Iteration 317, loss = 0.99875632\n",
            "Iteration 318, loss = 0.99868172\n",
            "Iteration 319, loss = 0.99851258\n",
            "Iteration 320, loss = 0.99833210\n",
            "Iteration 321, loss = 0.99818791\n",
            "Iteration 322, loss = 0.99804086\n",
            "Iteration 323, loss = 0.99793002\n",
            "Iteration 324, loss = 0.99773721\n",
            "Iteration 325, loss = 0.99760561\n",
            "Iteration 326, loss = 0.99749057\n",
            "Iteration 327, loss = 0.99736340\n",
            "Iteration 328, loss = 0.99719564\n",
            "Iteration 329, loss = 0.99711399\n",
            "Iteration 330, loss = 0.99696602\n",
            "Iteration 331, loss = 0.99689327\n",
            "Iteration 332, loss = 0.99671382\n",
            "Iteration 333, loss = 0.99660689\n",
            "Iteration 334, loss = 0.99631506\n",
            "Iteration 335, loss = 0.99619178\n",
            "Iteration 336, loss = 0.99601496\n",
            "Iteration 337, loss = 0.99581594\n",
            "Iteration 338, loss = 0.99558446\n",
            "Iteration 339, loss = 0.99539835\n",
            "Iteration 340, loss = 0.99520459\n",
            "Iteration 341, loss = 0.99492883\n",
            "Iteration 342, loss = 0.99467643\n",
            "Iteration 343, loss = 0.99437373\n",
            "Iteration 344, loss = 0.99419825\n",
            "Iteration 345, loss = 0.99378191\n",
            "Iteration 346, loss = 0.99347558\n",
            "Iteration 347, loss = 0.99319228\n",
            "Iteration 348, loss = 0.99279560\n",
            "Iteration 349, loss = 0.99249138\n",
            "Iteration 350, loss = 0.99218420\n",
            "Iteration 351, loss = 0.99178544\n",
            "Iteration 352, loss = 0.99139840\n",
            "Iteration 353, loss = 0.99099661\n",
            "Iteration 354, loss = 0.99061768\n",
            "Iteration 355, loss = 0.99005830\n",
            "Iteration 356, loss = 0.98964191\n",
            "Iteration 357, loss = 0.98912703\n",
            "Iteration 358, loss = 0.98870501\n",
            "Iteration 359, loss = 0.98815324\n",
            "Iteration 360, loss = 0.98769377\n",
            "Iteration 361, loss = 0.98717911\n",
            "Iteration 362, loss = 0.98677729\n",
            "Iteration 363, loss = 0.98629754\n",
            "Iteration 364, loss = 0.98586412\n",
            "Iteration 365, loss = 0.98536154\n",
            "Iteration 366, loss = 0.98498065\n",
            "Iteration 367, loss = 0.98453254\n",
            "Iteration 368, loss = 0.98391211\n",
            "Iteration 369, loss = 0.98341022\n",
            "Iteration 370, loss = 0.98290464\n",
            "Iteration 371, loss = 0.98241404\n",
            "Iteration 372, loss = 0.98196581\n",
            "Iteration 373, loss = 0.98147370\n",
            "Iteration 374, loss = 0.98090806\n",
            "Iteration 375, loss = 0.98043047\n",
            "Iteration 376, loss = 0.97989972\n",
            "Iteration 377, loss = 0.97948425\n",
            "Iteration 378, loss = 0.97897348\n",
            "Iteration 379, loss = 0.97855657\n",
            "Iteration 380, loss = 0.97807384\n",
            "Iteration 381, loss = 0.97764433\n",
            "Iteration 382, loss = 0.97721268\n",
            "Iteration 383, loss = 0.97678147\n",
            "Iteration 384, loss = 0.97636764\n",
            "Iteration 385, loss = 0.97600215\n",
            "Iteration 386, loss = 0.97552106\n",
            "Iteration 387, loss = 0.97517890\n",
            "Iteration 388, loss = 0.97473859\n",
            "Iteration 389, loss = 0.97427994\n",
            "Iteration 390, loss = 0.97389765\n",
            "Iteration 391, loss = 0.97342294\n",
            "Iteration 392, loss = 0.97304125\n",
            "Iteration 393, loss = 0.97255700\n",
            "Iteration 394, loss = 0.97214594\n",
            "Iteration 395, loss = 0.97168726\n",
            "Iteration 396, loss = 0.97132009\n",
            "Iteration 397, loss = 0.97091663\n",
            "Iteration 398, loss = 0.97042035\n",
            "Iteration 399, loss = 0.96998583\n",
            "Iteration 400, loss = 0.96953860\n",
            "Iteration 401, loss = 0.96919041\n",
            "Iteration 402, loss = 0.96876654\n",
            "Iteration 403, loss = 0.96826901\n",
            "Iteration 404, loss = 0.96792324\n",
            "Iteration 405, loss = 0.96752482\n",
            "Iteration 406, loss = 0.96732229\n",
            "Iteration 407, loss = 0.96690202\n",
            "Iteration 408, loss = 0.96649632\n",
            "Iteration 409, loss = 0.96623831\n",
            "Iteration 410, loss = 0.96575880\n",
            "Iteration 411, loss = 0.96539876\n",
            "Iteration 412, loss = 0.96505369\n",
            "Iteration 413, loss = 0.96474848\n",
            "Iteration 414, loss = 0.96455069\n",
            "Iteration 415, loss = 0.96411406\n",
            "Iteration 416, loss = 0.96378762\n",
            "Iteration 417, loss = 0.96345357\n",
            "Iteration 418, loss = 0.96316637\n",
            "Iteration 419, loss = 0.96300227\n",
            "Iteration 420, loss = 0.96259102\n",
            "Iteration 421, loss = 0.96236674\n",
            "Iteration 422, loss = 0.96197590\n",
            "Iteration 423, loss = 0.96166471\n",
            "Iteration 424, loss = 0.96137751\n",
            "Iteration 425, loss = 0.96124544\n",
            "Iteration 426, loss = 0.96090561\n",
            "Iteration 427, loss = 0.96058046\n",
            "Iteration 428, loss = 0.96019496\n",
            "Iteration 429, loss = 0.95996414\n",
            "Iteration 430, loss = 0.95969596\n",
            "Iteration 431, loss = 0.95940440\n",
            "Iteration 432, loss = 0.95917872\n",
            "Iteration 433, loss = 0.95890501\n",
            "Iteration 434, loss = 0.95862674\n",
            "Iteration 435, loss = 0.95840107\n",
            "Iteration 436, loss = 0.95807266\n",
            "Iteration 437, loss = 0.95796532\n",
            "Iteration 438, loss = 0.95772989\n",
            "Iteration 439, loss = 0.95742986\n",
            "Iteration 440, loss = 0.95718296\n",
            "Iteration 441, loss = 0.95694762\n",
            "Iteration 442, loss = 0.95675479\n",
            "Iteration 443, loss = 0.95652493\n",
            "Iteration 444, loss = 0.95646035\n",
            "Iteration 445, loss = 0.95627455\n",
            "Iteration 446, loss = 0.95592363\n",
            "Iteration 447, loss = 0.95585674\n",
            "Iteration 448, loss = 0.95549818\n",
            "Iteration 449, loss = 0.95534467\n",
            "Iteration 450, loss = 0.95518303\n",
            "Iteration 451, loss = 0.95504037\n",
            "Iteration 452, loss = 0.95482233\n",
            "Iteration 453, loss = 0.95466799\n",
            "Iteration 454, loss = 0.95455458\n",
            "Iteration 455, loss = 0.95432213\n",
            "Iteration 456, loss = 0.95411369\n",
            "Iteration 457, loss = 0.95393907\n",
            "Iteration 458, loss = 0.95376606\n",
            "Iteration 459, loss = 0.95364331\n",
            "Iteration 460, loss = 0.95343505\n",
            "Iteration 461, loss = 0.95325469\n",
            "Iteration 462, loss = 0.95306508\n",
            "Iteration 463, loss = 0.95286788\n",
            "Iteration 464, loss = 0.95263391\n",
            "Iteration 465, loss = 0.95253110\n",
            "Iteration 466, loss = 0.95230434\n",
            "Iteration 467, loss = 0.95214138\n",
            "Iteration 468, loss = 0.95196518\n",
            "Iteration 469, loss = 0.95180298\n",
            "Iteration 470, loss = 0.95170108\n",
            "Iteration 471, loss = 0.95150260\n",
            "Iteration 472, loss = 0.95127228\n",
            "Iteration 473, loss = 0.95103141\n",
            "Iteration 474, loss = 0.95090949\n",
            "Iteration 475, loss = 0.95081842\n",
            "Iteration 476, loss = 0.95069526\n",
            "Iteration 477, loss = 0.95056817\n",
            "Iteration 478, loss = 0.95041418\n",
            "Iteration 479, loss = 0.95030973\n",
            "Iteration 480, loss = 0.95010574\n",
            "Iteration 481, loss = 0.94993672\n",
            "Iteration 482, loss = 0.94980653\n",
            "Iteration 483, loss = 0.94960509\n",
            "Iteration 484, loss = 0.94947122\n",
            "Iteration 485, loss = 0.94933579\n",
            "Iteration 486, loss = 0.94916890\n",
            "Iteration 487, loss = 0.94904068\n",
            "Iteration 488, loss = 0.94894591\n",
            "Iteration 489, loss = 0.94872971\n",
            "Iteration 490, loss = 0.94851462\n",
            "Iteration 491, loss = 0.94839963\n",
            "Iteration 492, loss = 0.94829087\n",
            "Iteration 493, loss = 0.94810332\n",
            "Iteration 494, loss = 0.94802244\n",
            "Iteration 495, loss = 0.94786762\n",
            "Iteration 496, loss = 0.94766028\n",
            "Iteration 497, loss = 0.94758565\n",
            "Iteration 498, loss = 0.94748343\n",
            "Iteration 499, loss = 0.94726981\n",
            "Iteration 500, loss = 0.94721500\n",
            "Iteration 501, loss = 0.94703796\n",
            "Iteration 502, loss = 0.94688309\n",
            "Iteration 503, loss = 0.94677845\n",
            "Iteration 504, loss = 0.94663065\n",
            "Iteration 505, loss = 0.94656337\n",
            "Iteration 506, loss = 0.94649230\n",
            "Iteration 507, loss = 0.94630801\n",
            "Iteration 508, loss = 0.94616485\n",
            "Iteration 509, loss = 0.94602826\n",
            "Iteration 510, loss = 0.94587736\n",
            "Iteration 511, loss = 0.94577019\n",
            "Iteration 512, loss = 0.94568487\n",
            "Iteration 513, loss = 0.94555170\n",
            "Iteration 514, loss = 0.94538544\n",
            "Iteration 515, loss = 0.94529063\n",
            "Iteration 516, loss = 0.94517239\n",
            "Iteration 517, loss = 0.94499597\n",
            "Iteration 518, loss = 0.94495800\n",
            "Iteration 519, loss = 0.94483719\n",
            "Iteration 520, loss = 0.94465849\n",
            "Iteration 521, loss = 0.94456278\n",
            "Iteration 522, loss = 0.94446773\n",
            "Iteration 523, loss = 0.94432944\n",
            "Iteration 524, loss = 0.94426140\n",
            "Iteration 525, loss = 0.94414639\n",
            "Iteration 526, loss = 0.94408938\n",
            "Iteration 527, loss = 0.94393941\n",
            "Iteration 528, loss = 0.94381884\n",
            "Iteration 529, loss = 0.94375014\n",
            "Iteration 530, loss = 0.94368692\n",
            "Iteration 531, loss = 0.94357860\n",
            "Iteration 532, loss = 0.94342775\n",
            "Iteration 533, loss = 0.94356452\n",
            "Iteration 534, loss = 0.94331472\n",
            "Iteration 535, loss = 0.94313629\n",
            "Iteration 536, loss = 0.94304303\n",
            "Iteration 537, loss = 0.94296533\n",
            "Iteration 538, loss = 0.94288241\n",
            "Iteration 539, loss = 0.94281234\n",
            "Iteration 540, loss = 0.94273189\n",
            "Iteration 541, loss = 0.94265924\n",
            "Iteration 542, loss = 0.94254603\n",
            "Iteration 543, loss = 0.94248414\n",
            "Iteration 544, loss = 0.94242336\n",
            "Iteration 545, loss = 0.94241721\n",
            "Iteration 546, loss = 0.94215312\n",
            "Iteration 547, loss = 0.94210424\n",
            "Iteration 548, loss = 0.94202578\n",
            "Iteration 549, loss = 0.94191933\n",
            "Iteration 550, loss = 0.94180810\n",
            "Iteration 551, loss = 0.94173948\n",
            "Iteration 552, loss = 0.94167461\n",
            "Iteration 553, loss = 0.94158139\n",
            "Iteration 554, loss = 0.94150631\n",
            "Iteration 555, loss = 0.94143691\n",
            "Iteration 556, loss = 0.94133202\n",
            "Iteration 557, loss = 0.94124890\n",
            "Iteration 558, loss = 0.94118187\n",
            "Iteration 559, loss = 0.94118624\n",
            "Iteration 560, loss = 0.94109604\n",
            "Iteration 561, loss = 0.94103177\n",
            "Iteration 562, loss = 0.94086084\n",
            "Iteration 563, loss = 0.94083234\n",
            "Iteration 564, loss = 0.94079264\n",
            "Iteration 565, loss = 0.94064032\n",
            "Iteration 566, loss = 0.94048881\n",
            "Iteration 567, loss = 0.94044602\n",
            "Iteration 568, loss = 0.94038999\n",
            "Iteration 569, loss = 0.94030802\n",
            "Iteration 570, loss = 0.94025783\n",
            "Iteration 571, loss = 0.94016924\n",
            "Iteration 572, loss = 0.94017071\n",
            "Iteration 573, loss = 0.94011958\n",
            "Iteration 574, loss = 0.94009012\n",
            "Iteration 575, loss = 0.93988575\n",
            "Iteration 576, loss = 0.93989697\n",
            "Iteration 577, loss = 0.93978231\n",
            "Iteration 578, loss = 0.93976504\n",
            "Iteration 579, loss = 0.93972892\n",
            "Iteration 580, loss = 0.93965956\n",
            "Iteration 581, loss = 0.93957947\n",
            "Iteration 582, loss = 0.93951078\n",
            "Iteration 583, loss = 0.93946788\n",
            "Iteration 584, loss = 0.93943911\n",
            "Iteration 585, loss = 0.93946657\n",
            "Iteration 586, loss = 0.93939699\n",
            "Iteration 587, loss = 0.93932601\n",
            "Iteration 588, loss = 0.93919205\n",
            "Iteration 589, loss = 0.93915798\n",
            "Iteration 590, loss = 0.93910738\n",
            "Iteration 591, loss = 0.93902061\n",
            "Iteration 592, loss = 0.93896617\n",
            "Iteration 593, loss = 0.93897146\n",
            "Iteration 594, loss = 0.93887832\n",
            "Iteration 595, loss = 0.93881593\n",
            "Iteration 596, loss = 0.93876298\n",
            "Iteration 597, loss = 0.93869772\n",
            "Iteration 598, loss = 0.93872387\n",
            "Iteration 599, loss = 0.93859118\n",
            "Iteration 600, loss = 0.93851430\n",
            "Iteration 601, loss = 0.93846286\n",
            "Iteration 602, loss = 0.93839457\n",
            "Iteration 603, loss = 0.93834555\n",
            "Iteration 604, loss = 0.93833329\n",
            "Iteration 605, loss = 0.93830852\n",
            "Iteration 606, loss = 0.93832529\n",
            "Iteration 607, loss = 0.93815353\n",
            "Iteration 608, loss = 0.93816176\n",
            "Iteration 609, loss = 0.93813288\n",
            "Iteration 610, loss = 0.93802995\n",
            "Iteration 611, loss = 0.93796177\n",
            "Iteration 612, loss = 0.93789412\n",
            "Iteration 613, loss = 0.93788446\n",
            "Iteration 614, loss = 0.93773268\n",
            "Iteration 615, loss = 0.93775049\n",
            "Iteration 616, loss = 0.93764264\n",
            "Iteration 617, loss = 0.93762626\n",
            "Iteration 618, loss = 0.93752523\n",
            "Iteration 619, loss = 0.93754512\n",
            "Iteration 620, loss = 0.93744810\n",
            "Iteration 621, loss = 0.93737512\n",
            "Iteration 622, loss = 0.93731316\n",
            "Iteration 623, loss = 0.93734108\n",
            "Iteration 624, loss = 0.93722306\n",
            "Iteration 625, loss = 0.93719420\n",
            "Iteration 626, loss = 0.93722382\n",
            "Iteration 627, loss = 0.93714818\n",
            "Iteration 628, loss = 0.93708388\n",
            "Iteration 629, loss = 0.93701429\n",
            "Iteration 630, loss = 0.93702881\n",
            "Iteration 631, loss = 0.93696350\n",
            "Iteration 632, loss = 0.93697999\n",
            "Iteration 633, loss = 0.93687285\n",
            "Iteration 634, loss = 0.93693653\n",
            "Iteration 635, loss = 0.93678750\n",
            "Iteration 636, loss = 0.93678899\n",
            "Iteration 637, loss = 0.93682991\n",
            "Iteration 638, loss = 0.93676066\n",
            "Iteration 639, loss = 0.93670578\n",
            "Iteration 640, loss = 0.93668128\n",
            "Iteration 641, loss = 0.93662477\n",
            "Iteration 642, loss = 0.93661500\n",
            "Iteration 643, loss = 0.93652725\n",
            "Iteration 644, loss = 0.93651062\n",
            "Iteration 645, loss = 0.93642769\n",
            "Iteration 646, loss = 0.93655437\n",
            "Iteration 647, loss = 0.93644581\n",
            "Iteration 648, loss = 0.93637214\n",
            "Iteration 649, loss = 0.93635031\n",
            "Iteration 650, loss = 0.93629510\n",
            "Iteration 651, loss = 0.93626893\n",
            "Iteration 652, loss = 0.93621260\n",
            "Iteration 653, loss = 0.93616741\n",
            "Iteration 654, loss = 0.93612170\n",
            "Iteration 655, loss = 0.93616727\n",
            "Iteration 656, loss = 0.93611711\n",
            "Iteration 657, loss = 0.93603663\n",
            "Iteration 658, loss = 0.93601240\n",
            "Iteration 659, loss = 0.93606690\n",
            "Iteration 660, loss = 0.93599652\n",
            "Iteration 661, loss = 0.93598841\n",
            "Iteration 662, loss = 0.93589471\n",
            "Iteration 663, loss = 0.93595213\n",
            "Iteration 664, loss = 0.93594571\n",
            "Iteration 665, loss = 0.93588238\n",
            "Iteration 666, loss = 0.93583993\n",
            "Iteration 667, loss = 0.93579206\n",
            "Iteration 668, loss = 0.93588527\n",
            "Iteration 669, loss = 0.93590474\n",
            "Iteration 670, loss = 0.93581429\n",
            "Iteration 671, loss = 0.93571838\n",
            "Iteration 672, loss = 0.93571500\n",
            "Iteration 673, loss = 0.93568482\n",
            "Iteration 674, loss = 0.93568787\n",
            "Iteration 675, loss = 0.93571088\n",
            "Iteration 676, loss = 0.93564704\n",
            "Iteration 677, loss = 0.93567633\n",
            "Iteration 678, loss = 0.93557491\n",
            "Iteration 679, loss = 0.93559320\n",
            "Iteration 680, loss = 0.93567207\n",
            "Iteration 681, loss = 0.93554342\n",
            "Iteration 682, loss = 0.93555412\n",
            "Iteration 683, loss = 0.93554756\n",
            "Iteration 684, loss = 0.93552472\n",
            "Iteration 685, loss = 0.93548672\n",
            "Iteration 686, loss = 0.93550343\n",
            "Iteration 687, loss = 0.93542561\n",
            "Iteration 688, loss = 0.93540852\n",
            "Iteration 689, loss = 0.93539133\n",
            "Iteration 690, loss = 0.93539780\n",
            "Iteration 691, loss = 0.93539213\n",
            "Iteration 692, loss = 0.93540475\n",
            "Iteration 693, loss = 0.93555986\n",
            "Iteration 694, loss = 0.93528826\n",
            "Iteration 695, loss = 0.93533032\n",
            "Iteration 696, loss = 0.93544606\n",
            "Iteration 697, loss = 0.93530544\n",
            "Iteration 698, loss = 0.93532541\n",
            "Iteration 699, loss = 0.93525750\n",
            "Iteration 700, loss = 0.93533769\n",
            "Iteration 701, loss = 0.93523136\n",
            "Iteration 702, loss = 0.93523578\n",
            "Iteration 703, loss = 0.93524073\n",
            "Iteration 704, loss = 0.93521916\n",
            "Iteration 705, loss = 0.93519807\n",
            "Iteration 706, loss = 0.93514764\n",
            "Iteration 707, loss = 0.93518391\n",
            "Iteration 708, loss = 0.93513585\n",
            "Iteration 709, loss = 0.93522657\n",
            "Iteration 710, loss = 0.93513001\n",
            "Iteration 711, loss = 0.93513668\n",
            "Iteration 712, loss = 0.93508462\n",
            "Iteration 713, loss = 0.93508841\n",
            "Iteration 714, loss = 0.93509822\n",
            "Iteration 715, loss = 0.93509748\n",
            "Iteration 716, loss = 0.93511269\n",
            "Iteration 717, loss = 0.93497512\n",
            "Iteration 718, loss = 0.93501198\n",
            "Iteration 719, loss = 0.93497535\n",
            "Iteration 720, loss = 0.93495020\n",
            "Iteration 721, loss = 0.93493462\n",
            "Iteration 722, loss = 0.93495089\n",
            "Iteration 723, loss = 0.93489934\n",
            "Iteration 724, loss = 0.93492289\n",
            "Iteration 725, loss = 0.93495516\n",
            "Iteration 726, loss = 0.93491772\n",
            "Iteration 727, loss = 0.93481511\n",
            "Iteration 728, loss = 0.93485212\n",
            "Iteration 729, loss = 0.93487930\n",
            "Iteration 730, loss = 0.93486010\n",
            "Iteration 731, loss = 0.93487988\n",
            "Iteration 732, loss = 0.93486609\n",
            "Iteration 733, loss = 0.93483736\n",
            "Iteration 734, loss = 0.93478065\n",
            "Iteration 735, loss = 0.93474282\n",
            "Iteration 736, loss = 0.93485618\n",
            "Iteration 737, loss = 0.93478641\n",
            "Iteration 738, loss = 0.93473587\n",
            "Iteration 739, loss = 0.93471438\n",
            "Iteration 740, loss = 0.93468981\n",
            "Iteration 741, loss = 0.93466152\n",
            "Iteration 742, loss = 0.93471780\n",
            "Iteration 743, loss = 0.93474304\n",
            "Iteration 744, loss = 0.93474102\n",
            "Iteration 745, loss = 0.93475641\n",
            "Iteration 746, loss = 0.93465266\n",
            "Iteration 747, loss = 0.93458770\n",
            "Iteration 748, loss = 0.93465750\n",
            "Iteration 749, loss = 0.93462789\n",
            "Iteration 750, loss = 0.93464271\n",
            "Iteration 751, loss = 0.93464170\n",
            "Iteration 752, loss = 0.93461003\n",
            "Iteration 753, loss = 0.93459919\n",
            "Iteration 754, loss = 0.93463800\n",
            "Iteration 755, loss = 0.93462723\n",
            "Iteration 756, loss = 0.93456890\n",
            "Iteration 757, loss = 0.93464900\n",
            "Iteration 758, loss = 0.93457687\n",
            "Iteration 759, loss = 0.93453096\n",
            "Iteration 760, loss = 0.93456846\n",
            "Iteration 761, loss = 0.93461617\n",
            "Iteration 762, loss = 0.93454626\n",
            "Iteration 763, loss = 0.93454281\n",
            "Iteration 764, loss = 0.93446929\n",
            "Iteration 765, loss = 0.93449089\n",
            "Iteration 766, loss = 0.93449169\n",
            "Iteration 767, loss = 0.93446048\n",
            "Iteration 768, loss = 0.93459640\n",
            "Iteration 769, loss = 0.93444521\n",
            "Iteration 770, loss = 0.93449596\n",
            "Iteration 771, loss = 0.93444126\n",
            "Iteration 772, loss = 0.93442516\n",
            "Iteration 773, loss = 0.93450866\n",
            "Iteration 774, loss = 0.93445675\n",
            "Iteration 775, loss = 0.93444566\n",
            "Iteration 776, loss = 0.93443711\n",
            "Iteration 777, loss = 0.93442516\n",
            "Iteration 778, loss = 0.93440339\n",
            "Iteration 779, loss = 0.93434196\n",
            "Iteration 780, loss = 0.93431570\n",
            "Iteration 781, loss = 0.93429978\n",
            "Iteration 782, loss = 0.93434975\n",
            "Iteration 783, loss = 0.93426665\n",
            "Iteration 784, loss = 0.93426567\n",
            "Iteration 785, loss = 0.93431136\n",
            "Iteration 786, loss = 0.93425837\n",
            "Iteration 787, loss = 0.93424767\n",
            "Iteration 788, loss = 0.93424085\n",
            "Iteration 789, loss = 0.93433975\n",
            "Iteration 790, loss = 0.93424856\n",
            "Iteration 791, loss = 0.93441603\n",
            "Iteration 792, loss = 0.93424909\n",
            "Iteration 793, loss = 0.93427093\n",
            "Iteration 794, loss = 0.93419644\n",
            "Iteration 795, loss = 0.93418886\n",
            "Iteration 796, loss = 0.93420811\n",
            "Iteration 797, loss = 0.93419098\n",
            "Iteration 798, loss = 0.93415336\n",
            "Iteration 799, loss = 0.93430311\n",
            "Iteration 800, loss = 0.93410681\n",
            "Iteration 801, loss = 0.93413866\n",
            "Iteration 802, loss = 0.93416395\n",
            "Iteration 803, loss = 0.93411919\n",
            "Iteration 804, loss = 0.93403685\n",
            "Iteration 805, loss = 0.93398829\n",
            "Iteration 806, loss = 0.93411133\n",
            "Iteration 807, loss = 0.93405159\n",
            "Iteration 808, loss = 0.93399889\n",
            "Iteration 809, loss = 0.93403814\n",
            "Iteration 810, loss = 0.93403304\n",
            "Iteration 811, loss = 0.93399521\n",
            "Iteration 812, loss = 0.93400619\n",
            "Iteration 813, loss = 0.93401796\n",
            "Iteration 814, loss = 0.93399749\n",
            "Iteration 815, loss = 0.93387615\n",
            "Iteration 816, loss = 0.93390528\n",
            "Iteration 817, loss = 0.93395690\n",
            "Iteration 818, loss = 0.93395188\n",
            "Iteration 819, loss = 0.93391560\n",
            "Iteration 820, loss = 0.93394237\n",
            "Iteration 821, loss = 0.93392933\n",
            "Iteration 822, loss = 0.93395988\n",
            "Iteration 823, loss = 0.93384317\n",
            "Iteration 824, loss = 0.93388207\n",
            "Iteration 825, loss = 0.93381244\n",
            "Iteration 826, loss = 0.93382438\n",
            "Iteration 827, loss = 0.93385009\n",
            "Iteration 828, loss = 0.93387802\n",
            "Iteration 829, loss = 0.93376743\n",
            "Iteration 830, loss = 0.93371158\n",
            "Iteration 831, loss = 0.93374623\n",
            "Iteration 832, loss = 0.93374022\n",
            "Iteration 833, loss = 0.93370103\n",
            "Iteration 834, loss = 0.93368367\n",
            "Iteration 835, loss = 0.93367625\n",
            "Iteration 836, loss = 0.93368025\n",
            "Iteration 837, loss = 0.93370077\n",
            "Iteration 838, loss = 0.93364090\n",
            "Iteration 839, loss = 0.93365865\n",
            "Iteration 840, loss = 0.93365854\n",
            "Iteration 841, loss = 0.93362866\n",
            "Iteration 842, loss = 0.93360029\n",
            "Iteration 843, loss = 0.93360020\n",
            "Iteration 844, loss = 0.93365972\n",
            "Iteration 845, loss = 0.93357516\n",
            "Iteration 846, loss = 0.93356158\n",
            "Iteration 847, loss = 0.93356858\n",
            "Iteration 848, loss = 0.93363648\n",
            "Iteration 849, loss = 0.93352283\n",
            "Iteration 850, loss = 0.93351823\n",
            "Iteration 851, loss = 0.93357537\n",
            "Iteration 852, loss = 0.93353077\n",
            "Iteration 853, loss = 0.93350137\n",
            "Iteration 854, loss = 0.93353482\n",
            "Iteration 855, loss = 0.93348543\n",
            "Iteration 856, loss = 0.93349746\n",
            "Iteration 857, loss = 0.93348771\n",
            "Iteration 858, loss = 0.93345947\n",
            "Iteration 859, loss = 0.93342475\n",
            "Iteration 860, loss = 0.93345873\n",
            "Iteration 861, loss = 0.93342599\n",
            "Iteration 862, loss = 0.93345459\n",
            "Iteration 863, loss = 0.93352530\n",
            "Iteration 864, loss = 0.93340629\n",
            "Iteration 865, loss = 0.93340099\n",
            "Iteration 866, loss = 0.93352619\n",
            "Iteration 867, loss = 0.93345978\n",
            "Iteration 868, loss = 0.93344815\n",
            "Iteration 869, loss = 0.93347603\n",
            "Iteration 870, loss = 0.93358431\n",
            "Iteration 871, loss = 0.93333901\n",
            "Iteration 872, loss = 0.93338493\n",
            "Iteration 873, loss = 0.93337726\n",
            "Iteration 874, loss = 0.93335618\n",
            "Iteration 875, loss = 0.93329794\n",
            "Iteration 876, loss = 0.93335486\n",
            "Iteration 877, loss = 0.93329650\n",
            "Iteration 878, loss = 0.93329793\n",
            "Iteration 879, loss = 0.93331815\n",
            "Iteration 880, loss = 0.93328987\n",
            "Iteration 881, loss = 0.93333166\n",
            "Iteration 882, loss = 0.93332687\n",
            "Iteration 883, loss = 0.93331823\n",
            "Iteration 884, loss = 0.93328320\n",
            "Iteration 885, loss = 0.93330063\n",
            "Iteration 886, loss = 0.93334902\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "0.49184782608695654\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.49184782608695654"
            ]
          },
          "metadata": {},
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KFTCFFljlBwP",
        "outputId": "dd495762-434e-4c28-b159-e0f8607aec92"
      },
      "source": [
        "%reset -f\n",
        "import pickle\n",
        "with open('cmc.pkl', 'rb') as f:  \n",
        "  X_cmc_treinamento, y_cmc_treinamento, X_cmc_teste, y_cmc_teste = pickle.load(f)\n",
        "from sklearn.neural_network import MLPClassifier # Neurônios = (3+1)/2 = 2\n",
        "print(X_cmc_treinamento.shape) # Neurônios = (108 + 1)/2 = 55\n",
        "rede_neural_cmc = MLPClassifier(verbose=True, max_iter = 10000, tol=0.000001,\n",
        "                                  hidden_layer_sizes = (55,55,55,55))\n",
        "rede_neural_cmc.fit(X_cmc_treinamento, y_cmc_treinamento)\n",
        "previsoes = rede_neural_cmc.predict(X_cmc_teste)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(accuracy_score(y_cmc_teste, previsoes))\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm = ConfusionMatrix(rede_neural_cmc)\n",
        "cm.fit(X_cmc_treinamento, y_cmc_treinamento)\n",
        "cm.score(X_cmc_teste, y_cmc_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1104, 9)\n",
            "Iteration 1, loss = 1.11463309\n",
            "Iteration 2, loss = 1.05457010\n",
            "Iteration 3, loss = 1.02084952\n",
            "Iteration 4, loss = 0.99334855\n",
            "Iteration 5, loss = 0.97244046\n",
            "Iteration 6, loss = 0.95374145\n",
            "Iteration 7, loss = 0.93771702\n",
            "Iteration 8, loss = 0.92197084\n",
            "Iteration 9, loss = 0.90963051\n",
            "Iteration 10, loss = 0.89596330\n",
            "Iteration 11, loss = 0.88179619\n",
            "Iteration 12, loss = 0.87146444\n",
            "Iteration 13, loss = 0.86165761\n",
            "Iteration 14, loss = 0.85449478\n",
            "Iteration 15, loss = 0.84425088\n",
            "Iteration 16, loss = 0.83844251\n",
            "Iteration 17, loss = 0.83198850\n",
            "Iteration 18, loss = 0.82527130\n",
            "Iteration 19, loss = 0.81917133\n",
            "Iteration 20, loss = 0.81306711\n",
            "Iteration 21, loss = 0.80951515\n",
            "Iteration 22, loss = 0.80789136\n",
            "Iteration 23, loss = 0.80720428\n",
            "Iteration 24, loss = 0.80141582\n",
            "Iteration 25, loss = 0.79214291\n",
            "Iteration 26, loss = 0.78817401\n",
            "Iteration 27, loss = 0.78059262\n",
            "Iteration 28, loss = 0.77443606\n",
            "Iteration 29, loss = 0.77221814\n",
            "Iteration 30, loss = 0.76481749\n",
            "Iteration 31, loss = 0.76048889\n",
            "Iteration 32, loss = 0.75517418\n",
            "Iteration 33, loss = 0.75349928\n",
            "Iteration 34, loss = 0.74541389\n",
            "Iteration 35, loss = 0.74347865\n",
            "Iteration 36, loss = 0.73961658\n",
            "Iteration 37, loss = 0.73497494\n",
            "Iteration 38, loss = 0.73162499\n",
            "Iteration 39, loss = 0.73155249\n",
            "Iteration 40, loss = 0.72396196\n",
            "Iteration 41, loss = 0.72190057\n",
            "Iteration 42, loss = 0.71464812\n",
            "Iteration 43, loss = 0.71477106\n",
            "Iteration 44, loss = 0.70478375\n",
            "Iteration 45, loss = 0.70171842\n",
            "Iteration 46, loss = 0.69898073\n",
            "Iteration 47, loss = 0.70034204\n",
            "Iteration 48, loss = 0.70410980\n",
            "Iteration 49, loss = 0.70451200\n",
            "Iteration 50, loss = 0.68826833\n",
            "Iteration 51, loss = 0.67791709\n",
            "Iteration 52, loss = 0.67316563\n",
            "Iteration 53, loss = 0.67026358\n",
            "Iteration 54, loss = 0.66835975\n",
            "Iteration 55, loss = 0.66063135\n",
            "Iteration 56, loss = 0.65650501\n",
            "Iteration 57, loss = 0.65411219\n",
            "Iteration 58, loss = 0.65388173\n",
            "Iteration 59, loss = 0.64466172\n",
            "Iteration 60, loss = 0.65859818\n",
            "Iteration 61, loss = 0.63648935\n",
            "Iteration 62, loss = 0.63474993\n",
            "Iteration 63, loss = 0.63268033\n",
            "Iteration 64, loss = 0.63025025\n",
            "Iteration 65, loss = 0.62234895\n",
            "Iteration 66, loss = 0.62529920\n",
            "Iteration 67, loss = 0.62503252\n",
            "Iteration 68, loss = 0.62459933\n",
            "Iteration 69, loss = 0.62824388\n",
            "Iteration 70, loss = 0.60689995\n",
            "Iteration 71, loss = 0.60162434\n",
            "Iteration 72, loss = 0.60166598\n",
            "Iteration 73, loss = 0.59734213\n",
            "Iteration 74, loss = 0.59873379\n",
            "Iteration 75, loss = 0.58521274\n",
            "Iteration 76, loss = 0.58409205\n",
            "Iteration 77, loss = 0.57629143\n",
            "Iteration 78, loss = 0.57549902\n",
            "Iteration 79, loss = 0.56685951\n",
            "Iteration 80, loss = 0.57201272\n",
            "Iteration 81, loss = 0.55863125\n",
            "Iteration 82, loss = 0.55993401\n",
            "Iteration 83, loss = 0.57867764\n",
            "Iteration 84, loss = 0.56370161\n",
            "Iteration 85, loss = 0.56498123\n",
            "Iteration 86, loss = 0.55987677\n",
            "Iteration 87, loss = 0.56082042\n",
            "Iteration 88, loss = 0.54396321\n",
            "Iteration 89, loss = 0.54138525\n",
            "Iteration 90, loss = 0.53343389\n",
            "Iteration 91, loss = 0.52879237\n",
            "Iteration 92, loss = 0.53667503\n",
            "Iteration 93, loss = 0.52365507\n",
            "Iteration 94, loss = 0.51906556\n",
            "Iteration 95, loss = 0.52064090\n",
            "Iteration 96, loss = 0.51456221\n",
            "Iteration 97, loss = 0.51270822\n",
            "Iteration 98, loss = 0.51000072\n",
            "Iteration 99, loss = 0.50432090\n",
            "Iteration 100, loss = 0.50029879\n",
            "Iteration 101, loss = 0.50415437\n",
            "Iteration 102, loss = 0.49663462\n",
            "Iteration 103, loss = 0.49218807\n",
            "Iteration 104, loss = 0.49300061\n",
            "Iteration 105, loss = 0.48544160\n",
            "Iteration 106, loss = 0.48117564\n",
            "Iteration 107, loss = 0.48053093\n",
            "Iteration 108, loss = 0.48179563\n",
            "Iteration 109, loss = 0.48249291\n",
            "Iteration 110, loss = 0.47417533\n",
            "Iteration 111, loss = 0.46505822\n",
            "Iteration 112, loss = 0.46283792\n",
            "Iteration 113, loss = 0.46188715\n",
            "Iteration 114, loss = 0.46678898\n",
            "Iteration 115, loss = 0.45275379\n",
            "Iteration 116, loss = 0.44655263\n",
            "Iteration 117, loss = 0.44906790\n",
            "Iteration 118, loss = 0.45014538\n",
            "Iteration 119, loss = 0.44486952\n",
            "Iteration 120, loss = 0.44654102\n",
            "Iteration 121, loss = 0.44619544\n",
            "Iteration 122, loss = 0.44289292\n",
            "Iteration 123, loss = 0.43826321\n",
            "Iteration 124, loss = 0.44609100\n",
            "Iteration 125, loss = 0.42756944\n",
            "Iteration 126, loss = 0.42444921\n",
            "Iteration 127, loss = 0.43509299\n",
            "Iteration 128, loss = 0.42393259\n",
            "Iteration 129, loss = 0.42445795\n",
            "Iteration 130, loss = 0.41773062\n",
            "Iteration 131, loss = 0.41896856\n",
            "Iteration 132, loss = 0.43159851\n",
            "Iteration 133, loss = 0.42764356\n",
            "Iteration 134, loss = 0.44039090\n",
            "Iteration 135, loss = 0.41929557\n",
            "Iteration 136, loss = 0.41030164\n",
            "Iteration 137, loss = 0.40422119\n",
            "Iteration 138, loss = 0.39847151\n",
            "Iteration 139, loss = 0.39846751\n",
            "Iteration 140, loss = 0.39399315\n",
            "Iteration 141, loss = 0.39555637\n",
            "Iteration 142, loss = 0.39007075\n",
            "Iteration 143, loss = 0.39450930\n",
            "Iteration 144, loss = 0.39268413\n",
            "Iteration 145, loss = 0.38922728\n",
            "Iteration 146, loss = 0.37578591\n",
            "Iteration 147, loss = 0.37283182\n",
            "Iteration 148, loss = 0.37467736\n",
            "Iteration 149, loss = 0.36938869\n",
            "Iteration 150, loss = 0.37550668\n",
            "Iteration 151, loss = 0.37229809\n",
            "Iteration 152, loss = 0.37320787\n",
            "Iteration 153, loss = 0.36638643\n",
            "Iteration 154, loss = 0.37193148\n",
            "Iteration 155, loss = 0.37623950\n",
            "Iteration 156, loss = 0.37150300\n",
            "Iteration 157, loss = 0.35923036\n",
            "Iteration 158, loss = 0.35700439\n",
            "Iteration 159, loss = 0.35613372\n",
            "Iteration 160, loss = 0.35736795\n",
            "Iteration 161, loss = 0.35103730\n",
            "Iteration 162, loss = 0.35017026\n",
            "Iteration 163, loss = 0.35370846\n",
            "Iteration 164, loss = 0.34747143\n",
            "Iteration 165, loss = 0.34807332\n",
            "Iteration 166, loss = 0.34067730\n",
            "Iteration 167, loss = 0.34017553\n",
            "Iteration 168, loss = 0.33445365\n",
            "Iteration 169, loss = 0.33700638\n",
            "Iteration 170, loss = 0.33514703\n",
            "Iteration 171, loss = 0.33599089\n",
            "Iteration 172, loss = 0.34219907\n",
            "Iteration 173, loss = 0.33409566\n",
            "Iteration 174, loss = 0.34093693\n",
            "Iteration 175, loss = 0.33833182\n",
            "Iteration 176, loss = 0.33238451\n",
            "Iteration 177, loss = 0.32536262\n",
            "Iteration 178, loss = 0.32897672\n",
            "Iteration 179, loss = 0.32804495\n",
            "Iteration 180, loss = 0.33634698\n",
            "Iteration 181, loss = 0.32851128\n",
            "Iteration 182, loss = 0.33953171\n",
            "Iteration 183, loss = 0.32658440\n",
            "Iteration 184, loss = 0.34331402\n",
            "Iteration 185, loss = 0.33910714\n",
            "Iteration 186, loss = 0.33317944\n",
            "Iteration 187, loss = 0.32876062\n",
            "Iteration 188, loss = 0.32029698\n",
            "Iteration 189, loss = 0.31578432\n",
            "Iteration 190, loss = 0.30787431\n",
            "Iteration 191, loss = 0.30725255\n",
            "Iteration 192, loss = 0.30523074\n",
            "Iteration 193, loss = 0.30346245\n",
            "Iteration 194, loss = 0.30523163\n",
            "Iteration 195, loss = 0.29903523\n",
            "Iteration 196, loss = 0.31239738\n",
            "Iteration 197, loss = 0.29962095\n",
            "Iteration 198, loss = 0.30571014\n",
            "Iteration 199, loss = 0.29975102\n",
            "Iteration 200, loss = 0.30612386\n",
            "Iteration 201, loss = 0.30340513\n",
            "Iteration 202, loss = 0.30377560\n",
            "Iteration 203, loss = 0.29969427\n",
            "Iteration 204, loss = 0.28842271\n",
            "Iteration 205, loss = 0.28448049\n",
            "Iteration 206, loss = 0.28489217\n",
            "Iteration 207, loss = 0.29069218\n",
            "Iteration 208, loss = 0.28828621\n",
            "Iteration 209, loss = 0.28926895\n",
            "Iteration 210, loss = 0.28896465\n",
            "Iteration 211, loss = 0.28587926\n",
            "Iteration 212, loss = 0.28390576\n",
            "Iteration 213, loss = 0.27643862\n",
            "Iteration 214, loss = 0.27897366\n",
            "Iteration 215, loss = 0.27555838\n",
            "Iteration 216, loss = 0.27683533\n",
            "Iteration 217, loss = 0.26998192\n",
            "Iteration 218, loss = 0.27256530\n",
            "Iteration 219, loss = 0.27874698\n",
            "Iteration 220, loss = 0.27989602\n",
            "Iteration 221, loss = 0.27686455\n",
            "Iteration 222, loss = 0.28518171\n",
            "Iteration 223, loss = 0.28583045\n",
            "Iteration 224, loss = 0.30855044\n",
            "Iteration 225, loss = 0.30741794\n",
            "Iteration 226, loss = 0.31364156\n",
            "Iteration 227, loss = 0.31697605\n",
            "Iteration 228, loss = 0.31655915\n",
            "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
            "0.49728260869565216\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.49728260869565216"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWkAAAD0CAYAAABZ9NdnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASoElEQVR4nO3ceXhV9YGH8TcbgSRACAQEUcQiR0WrGLW4QHHDqnVHmcG1xUrrSqkiVkfrtEXF1lpxXAtapT6DRVvE0qKoKBRobWpdUA+gSEDZQgJCCAlZ5g8sw2gCMZOb86u8n7+Sc/Mcvg+HvM/NuTek1dfXI0kKU3rSAyRJjTPSkhQwIy1JATPSkhQwIy1JActsyZMVFxdnA0cAK4Haljy3JH2JZQDdgdeKioqqdnygRSPNtkDPaeFzStLuYiAwd8cDLR3plQB/HvEjtqwpa+FTK9WuXfoS9UvvSHqGmimt91hWDTg66RlqhoJXZ7No0SL4tKE7aulI1wJsWVNG5crSFj61Ui07O5v69M1Jz1AzpWVnk7FmTdIz1Axt2rT554efu03sC4eSFDAjLUkBM9KSFDAjLUkBM9KSFDAjLUkBM9KSFDAjLUkBM9KSFDAjLUkBM9KSFDAjLUkBM9KSFDAjLUkBM9KSFDAjLUkBM9KSFDAjLUkBM9KSFDAjLUkBM9KSFDAjLUkBM9KSFDAjLUkBM9KSFDAjLUkBM9KSFDAjLUkBy0x6QMj6f3soX73ojO2f9zj8IJ4YMoKT7x5LbfVWSuYW89JNv0hwoXZmzPjZzC1eQU1NHWNHDuCcIX0BmDlnKadc9lvq4jEJL1Rjso8aQMFDD1ITLwJg63vvsfn3v6fjzTdRX1MDVdWUXXMtdWVlCS9NvSZFOoqig4BpwC/iOL4vtZPC8fqkqbw+aSoAvQYdQb/zT+G0B37E0/8+mtJ33+eMX/2Unkf1Z8X81xNeqs96ecEyFi4uZd6UC1lXXslhZz/GOUP6sqWqhjseXkD3wtykJ2oXqhYsoOzy727/vOChByi79vvUlpTQ/vujyL1gOBsnfPlztMvbHVEU5QITgBdTPydcg265kld+fD/tuxdS+u77ACyZOZevDDkm4WVqyKAj9uKpX277KSi/QzYVlVupra1j3IPzuWJ4f9q0yUh4ob6ospHfo7akBICMPfagduXKhBe1jqbck64CTgU+TvGWYPU4/GA+Wb6SitWllC9dwd4DDwfgKycdTV63LgmvU0MyMtLJzWkDwMSpb3LqoH15v2Q9b763lvNO2T/hdWqKrP32o/Ojkyj83dNkDxwIQPbgwXSb8woZhYVsfvqZhBe2jl1GOo7jmjiOK1tjTKgOu2wo/3jsdwA8O+Imvn7rVVw4cyKV5Z9AWlrC67Qz02YtZtLUt5hwy0mMvv0lfn7jcUlPUhNsXfohn/ziHtZ969uUjRpNp5/fBVlZVM2ezeqBX6dmyRLaX3Vl0jNbhe/uaIJeg7/G8nnb7juvXbiYJ068lMknj6D8/RLWf7gi4XVqzMw5Sxn34HxmPDKUTZuree+DMi687jmOOv8JVq6pYPCFTyY9UY2oW7WKymenA1C7bBl1a9aSd+kl2x/fPOOPtDniiKTmtSojvQt53btSvamCuq1bAThj4ji6HhyRlp7OVy86k0XPzU52oBq0YWMVY8bPZvpD51KQ3449u7VnyazLmf/URcx/6iK6d81l9uThSc9UI9qdfRZ5I0cCkF5YSHphF3IvGE5WvwMBaNP/UGo++CDJia3Gt+DtQvvuhVSs+d+3+bw+cSpnPXY7AG89+RxrFy5Oapp2YsqMdykt38ywUc9uP/brO09j7x4dElylptry/AsU/NcE2p08hLSsLNbf+ENqS9eRP+6nUFNL/ZYtlF1zbdIzW0VafX39Tr8giqIi4OfAPsBW4CPgnDiOP/cGxeLi4n2ApS+efg2VK0tbfKxS69b6mPpFNyQ9Q82U1vdOVuy5V9Iz1AyFHyzh7bffBuhdVFT04Y6P7fKZdBzHxcDglCyTJO2U96QlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWCZqTjpox3LWL1lbSpOrRS6FUjre2fSM/T/0POj5UlPUDNUVVU1+lhKIv367/6N7PTNqTi1UqigoIDSm/slPUPNlD56DnV3D0x6hprjylmNPuTtDkkKmJGWpIAZaUkKmJGWpIAZaUkKmJGWpIAZaUkKmJGWpIAZaUkKmJGWpIAZaUkKmJGWpIAZaUkKmJGWpIAZaUkKmJGWpIAZaUkKmJGWpIAZaUkKmJGWpIAZaUkKmJGWpIAZaUkKmJGWpIAZaUkKmJGWpIAZaUkKmJGWpIAZaUkKmJGWpIBlJj0gdGPGz2Zu8QpqauoYO3IARxy8BxeP+QO1tfV0L8zl8btOI7uNf42h2Vxdy7f+ezFrNlazpaaOm07amw7ZGdw8YxlZGWnktEnn8eERnXK8diFq6Pr16dyW705dQhqwX2E77j+3D5kZaUlPTbkm/QuNomg8MPDTr789juNnUroqEC8vWMbCxaXMm3Ih68orOezsxzjhqF5cMbw/552yPz+8+1UmTX2L7w3vn/RUfcb0hWUc3jOP64/vybKyLZz80Nt0aJvJExf0Jeqaw+2zlvPw/JXccMJeSU9VAxq6fvt3y+GG43tyygEF/OSFEp56Yy3DD+ua9NSU2+XtjiiKjgMOiuP4KOAbwD0pXxWIQUfsxVO/PAOA/A7ZVFRuZfZflnPGCX0AOP24r/Di/GVJTlQjhvUv5PrjewKwfH0VPfOz6ZybybqKGgDKK2vonJuV5ETtREPXb0lpJUfu3R6AIVEnXojXJzmx1TTlmfSrwF8//Xg9kBtFUUYcx7WpmxWGjIx0cnPaADBx6pucOmhfZs79cPvtja6dc1i5dlOSE7ULx977Bis2VPPsiAPJykjjuPvfolO7TDq1y2TcqfskPU+7sOP1GzdrOX94t4yLD+/G83E5qzdtTXpeq9hlpD+NccWnn44AZuwOgd7RtFmLmTT1LWZOOp++Qx7Zfry+PsFRapK51xzCPz7axMVPxhTmZvH0pQdwTO8OXP/sUh6Yt5KrB/ZIeqJ2YsfrN31EP654egmPv7aGQft2pH43+QZs8qsmURSdybZID0ndnPDMnLOUcQ/O54+/Oo+O7bPJy8micstW2rXN4qPVG+nRNS/piWpA8fJNdM3LYq9O2Ry6Zx41dfW8/P4GXujdAYAT++bz5N/XJLxSjWno+mVnpjH9sn4AzHyvnJUbqxNe2Tqa9Ba8KIpOBm4CTonjeENqJ4Vjw8YqxoyfzfSHzqUgvx0AJxzdi6dnLgLg6ecXcfLA3klOVCPmfLCBu1/5CIDVG6vZVFXHQXvk8M6qzQD8bflG9itsl+RE7URD1+++uSv5wztlADz22mq+eWBBkhNbzS6fSUdR1BG4CzgxjuOy1E8Kx5QZ71Javplho57dfuyxO07lOzf/iYenvEGvHh245KyDElyoxow8eg8um7KEr9/3JpVb65hwzr50zsli5G8Xk5WRRqecLCYO2y/pmWpEQ9evb2E7LnlyEf/5fAnH9u7AaUZ6u2FAF+CpKIr+eeziOI5LUrYqEJcPO5TLhx36uePPPzosgTX6ItplZfCbC6PPHZ9z9SEJrNEX1dj1WzDq89+PX3ZNeeHwYeDhVtgiSfoMfy1ckgJmpCUpYEZakgJmpCUpYEZakgJmpCUpYEZakgJmpCUpYEZakgJmpCUpYEZakgJmpCUpYEZakgJmpCUpYEZakgJmpCUpYEZakgJmpCUpYEZakgJmpCUpYEZakgJmpCUpYEZakgJmpCUpYEZakgJmpCUpYEZakgJmpCUpYEZakgJmpCUpYJmpOGla77GkZWen4tRKqUfo8pOFSY9QM5WNhvTRc5Keoeaoqmr0oZREuv7Fy6ivXZ+KUyuFysrKqF90Q9Iz1EwFBQVMLS9Meoaa4cr905k8eXKDj3m7Q5ICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWBGWpICZqQlKWCZSQ8I3ZhH32HuO2XU1NYzdmgfunRow01PvEdWZjq52Rk8PvpQOuW1SXqmGjBm/GzmFq+gpqaOsSMHcM6Qvtz7eDHX3fkyZX+9hrxcr1vIug0/nb3HXEZ9TQ1Lb7mXsuf/zAG/voOcPr2o2VjB20OvoWb9J0nPTLldRjqKohzgMaAb0Bb4cRzHz6V4VxBefrOUhSUbmXfXsaz7pJrDRr1K1/xsJo/uT9Qzj3FPLeahP5UwdmifpKfqM15esIyFi0uZN+VC1pVXctjZj7FpczWr11XQo2te0vO0C5kF+fS+9UpeKzqXjLwcet92NW177cnWteX87YLr6PGd88kfeDil019KemrKNeWZ9OnA3+I4Hh9FUS/gBWC3iPSgfp05sm8+APm5WVRU1dIpN4t1G6sBKK/YSrSn3/AhGnTEXhz51e4A5HfIpqJyK2eesB8d22fz5PR3El6nXSk48SjKZs2ndlMFtZsqiEfewiEzHuGDW+8F4ONHnkp4YevZZaTjOJ6yw6d7AStSNycsGRlp5GZs+yua+EIJpxZ15cbz+jD4h/PplJdFp9wsbr94/4RXqiEZGenk5my7nTFx6pucOmhfOrbPTniVmqrtPj3JyGnLwdMeIKtTB5b+aAJt99mTzqcMos/466leVUp8xW3UlG9IemrKNfmFwyiK5gFPAqNSNydM0xasYtKs5UwYeRDXPLyQZ248nPceOI5jDizg/hnLkp6nnZg2azGTpr7FhFtOSnqKvoC0NMjqnM/b51zFu5eO5YBHb4f0dDbHS3n9uIupeHsxvW4cmfTMVtHkSMdxfDRwBjA5iqK01E0Ky8y/r2Hcbxcz49Yj6ZibxZsffsIxBxYAcNKhXShesj7hhWrMzDlLGffgfGY8MtRn0f9iqlevY8O816mvraXyg+XUbKyAujrWv/IaAOtmziW33+7xWtAuIx1FUVEURXsBxHH8D7bdIilM9bAQbKjYyphH32X6fxxJQfttPzrvkZ/NOyUbAXht8Qb6dM9NcqIasWFjFWPGz2b6Q+dSkN8u6Tn6gtY9P5dOxw+AtDQyC/LJzMth1RPT6PyNgQC0L+rH5nhpwitbR1NeOBwE9AJGRVHUDcgDSlO6KhBT5nxM6cZqho0v3n5swsiDuPy+N8nKTKMgrw0TrzkkwYVqzJQZ71Javplho57dfmzw1/Zm9l9KWLW2glO/M5UBh/Zg/JjByY1Uo6o/XsOaqTM5fMG2FwgXXf0TymbN48Bf30n3EUOp3bSZdy65IeGVrSOtvr5+p18QRVE7YCLbXjRsB9wWx/H0hr62uLh4H2Bpv9X3kF3rbYB/NWmnT6d+0e7xD//LqPOAR5havlv8kPulc+X+6UyePBmgd1FR0Yc7PtaUd3dUAsNTM02StDP+WrgkBcxIS1LAjLQkBcxIS1LAjLQkBcxIS1LAjLQkBcxIS1LAjLQkBcxIS1LAjLQkBcxIS1LAjLQkBcxIS1LAjLQkBcxIS1LAjLQkBcxIS1LAjLQkBcxIS1LAjLQkBcxIS1LAjLQkBcxIS1LAjLQkBcxIS1LAjLQkBcxIS1LAjLQkBcxIS1LAMlv4fBkA1ekdWvi0ag1pVVXU1+UkPUPN1K1bN9LaFiQ9Q83Qpcv258sZn30srb6+vsX+oOLi4mOBOS12QknavQwsKiqau+OBln4m/RowEFgJ1LbwuSXpyyoD6M62hv4fLfpMWpLUsnzhUJICZqQlKWBGWpICZqQlKWBGWpICZqSbIYqi/KQ3qGmiKEpr4FjPJLao+aIo6pL0hqS09PukdxfPAMcnPUKNi6LobOAeICeKohnAVXEcb/z04cfx+gUriqLTgLuB5cAo4DdAZhRFucAVcRzPSHJfazPSjYii6IpGHkoD9mzNLWqWsUB/YD1wGfB8FEXfiON4A9uuocJ1M3ASsDfwHHBmHMdvRFHUDZgOGGkBMBqYxbbfnvysrFbeoi+uNo7jsk8/fjiKotXAzCiKvgn4G1xhq4rjuAQoiaLooziO3wCI43h1FEVbEt7W6ox0484C7gWujeO4ascHoiganMgifRFzoyh6DjgvjuPKOI6nffoN/iLQOeFt2rnVURRdF8fxz+I4Pga2v47wA7bdAtmt+MJhI+I4fhv4JrC1gYd/0Mpz9AXFcTwG+BmwZYdjM9n2f8vcltQuNcmlQMlnjnUFlgEjWn1Nwvy/OyQpYD6TlqSAGWlJCpiRlqSAGWlJCpiRlqSA/Q+Plft/sEuIWAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}