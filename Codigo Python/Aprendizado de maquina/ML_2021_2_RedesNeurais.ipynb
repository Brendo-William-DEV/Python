{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-2021-2-RedesNeurais.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y-mHL4_diOab",
        "outputId": "fc0b5fd0-a06e-4e85-8323-a7dfcd4ae199"
      },
      "source": [
        "%reset -f\n",
        "import pickle\n",
        "with open('credit.pkl', 'rb') as f:  \n",
        "  X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)\n",
        "from sklearn.neural_network import MLPClassifier # Neur√¥nios = (3+1)/2 = 2\n",
        "print(X_credit_treinamento.shape)\n",
        "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
        "                                   solver = 'adam', activation = 'relu',\n",
        "                                   hidden_layer_sizes = (2,2))\n",
        "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)\n",
        "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(accuracy_score(y_credit_teste, previsoes))\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm = ConfusionMatrix(rede_neural_credit)\n",
        "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
        "cm.score(X_credit_teste, y_credit_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1500, 3)\n",
            "Iteration 1, loss = 1.35580015\n",
            "Iteration 2, loss = 1.32474594\n",
            "Iteration 3, loss = 1.29488242\n",
            "Iteration 4, loss = 1.26543231\n",
            "Iteration 5, loss = 1.23761877\n",
            "Iteration 6, loss = 1.21031534\n",
            "Iteration 7, loss = 1.18429929\n",
            "Iteration 8, loss = 1.15936482\n",
            "Iteration 9, loss = 1.13516104\n",
            "Iteration 10, loss = 1.11235077\n",
            "Iteration 11, loss = 1.09012810\n",
            "Iteration 12, loss = 1.06884450\n",
            "Iteration 13, loss = 1.04859481\n",
            "Iteration 14, loss = 1.02845091\n",
            "Iteration 15, loss = 1.00952039\n",
            "Iteration 16, loss = 0.99081798\n",
            "Iteration 17, loss = 0.97296815\n",
            "Iteration 18, loss = 0.95552621\n",
            "Iteration 19, loss = 0.93878815\n",
            "Iteration 20, loss = 0.92265659\n",
            "Iteration 21, loss = 0.90719236\n",
            "Iteration 22, loss = 0.89232306\n",
            "Iteration 23, loss = 0.87773993\n",
            "Iteration 24, loss = 0.86356906\n",
            "Iteration 25, loss = 0.84991515\n",
            "Iteration 26, loss = 0.83681775\n",
            "Iteration 27, loss = 0.82438351\n",
            "Iteration 28, loss = 0.81245349\n",
            "Iteration 29, loss = 0.80087605\n",
            "Iteration 30, loss = 0.78966796\n",
            "Iteration 31, loss = 0.77864244\n",
            "Iteration 32, loss = 0.76798423\n",
            "Iteration 33, loss = 0.75780892\n",
            "Iteration 34, loss = 0.74794155\n",
            "Iteration 35, loss = 0.73856057\n",
            "Iteration 36, loss = 0.72940843\n",
            "Iteration 37, loss = 0.72054438\n",
            "Iteration 38, loss = 0.71200777\n",
            "Iteration 39, loss = 0.70352265\n",
            "Iteration 40, loss = 0.69548300\n",
            "Iteration 41, loss = 0.68774523\n",
            "Iteration 42, loss = 0.68024454\n",
            "Iteration 43, loss = 0.67311638\n",
            "Iteration 44, loss = 0.66614010\n",
            "Iteration 45, loss = 0.65948423\n",
            "Iteration 46, loss = 0.65296784\n",
            "Iteration 47, loss = 0.64683438\n",
            "Iteration 48, loss = 0.64077343\n",
            "Iteration 49, loss = 0.63494875\n",
            "Iteration 50, loss = 0.62925065\n",
            "Iteration 51, loss = 0.62372792\n",
            "Iteration 52, loss = 0.61830516\n",
            "Iteration 53, loss = 0.61296427\n",
            "Iteration 54, loss = 0.60784220\n",
            "Iteration 55, loss = 0.60283937\n",
            "Iteration 56, loss = 0.59798822\n",
            "Iteration 57, loss = 0.59322433\n",
            "Iteration 58, loss = 0.58859902\n",
            "Iteration 59, loss = 0.58401936\n",
            "Iteration 60, loss = 0.57958694\n",
            "Iteration 61, loss = 0.57518290\n",
            "Iteration 62, loss = 0.57096087\n",
            "Iteration 63, loss = 0.56698792\n",
            "Iteration 64, loss = 0.56301320\n",
            "Iteration 65, loss = 0.55922151\n",
            "Iteration 66, loss = 0.55552205\n",
            "Iteration 67, loss = 0.55198959\n",
            "Iteration 68, loss = 0.54857391\n",
            "Iteration 69, loss = 0.54529090\n",
            "Iteration 70, loss = 0.54204621\n",
            "Iteration 71, loss = 0.53883979\n",
            "Iteration 72, loss = 0.53578642\n",
            "Iteration 73, loss = 0.53281556\n",
            "Iteration 74, loss = 0.52991102\n",
            "Iteration 75, loss = 0.52706193\n",
            "Iteration 76, loss = 0.52425870\n",
            "Iteration 77, loss = 0.52167479\n",
            "Iteration 78, loss = 0.51895164\n",
            "Iteration 79, loss = 0.51638488\n",
            "Iteration 80, loss = 0.51387875\n",
            "Iteration 81, loss = 0.51137107\n",
            "Iteration 82, loss = 0.50890102\n",
            "Iteration 83, loss = 0.50641854\n",
            "Iteration 84, loss = 0.50399653\n",
            "Iteration 85, loss = 0.50153588\n",
            "Iteration 86, loss = 0.49927838\n",
            "Iteration 87, loss = 0.49708037\n",
            "Iteration 88, loss = 0.49491965\n",
            "Iteration 89, loss = 0.49277553\n",
            "Iteration 90, loss = 0.49075850\n",
            "Iteration 91, loss = 0.48876555\n",
            "Iteration 92, loss = 0.48682898\n",
            "Iteration 93, loss = 0.48491311\n",
            "Iteration 94, loss = 0.48313309\n",
            "Iteration 95, loss = 0.48135008\n",
            "Iteration 96, loss = 0.47956175\n",
            "Iteration 97, loss = 0.47790103\n",
            "Iteration 98, loss = 0.47624394\n",
            "Iteration 99, loss = 0.47464427\n",
            "Iteration 100, loss = 0.47306828\n",
            "Iteration 101, loss = 0.47152192\n",
            "Iteration 102, loss = 0.47002198\n",
            "Iteration 103, loss = 0.46848018\n",
            "Iteration 104, loss = 0.46706277\n",
            "Iteration 105, loss = 0.46565495\n",
            "Iteration 106, loss = 0.46425318\n",
            "Iteration 107, loss = 0.46288174\n",
            "Iteration 108, loss = 0.46153241\n",
            "Iteration 109, loss = 0.46017586\n",
            "Iteration 110, loss = 0.45884324\n",
            "Iteration 111, loss = 0.45757087\n",
            "Iteration 112, loss = 0.45627417\n",
            "Iteration 113, loss = 0.45506648\n",
            "Iteration 114, loss = 0.45388765\n",
            "Iteration 115, loss = 0.45267996\n",
            "Iteration 116, loss = 0.45152050\n",
            "Iteration 117, loss = 0.45042826\n",
            "Iteration 118, loss = 0.44936668\n",
            "Iteration 119, loss = 0.44830123\n",
            "Iteration 120, loss = 0.44729097\n",
            "Iteration 121, loss = 0.44624624\n",
            "Iteration 122, loss = 0.44527787\n",
            "Iteration 123, loss = 0.44430248\n",
            "Iteration 124, loss = 0.44336712\n",
            "Iteration 125, loss = 0.44243338\n",
            "Iteration 126, loss = 0.44150386\n",
            "Iteration 127, loss = 0.44058241\n",
            "Iteration 128, loss = 0.43969112\n",
            "Iteration 129, loss = 0.43881548\n",
            "Iteration 130, loss = 0.43791920\n",
            "Iteration 131, loss = 0.43707718\n",
            "Iteration 132, loss = 0.43629625\n",
            "Iteration 133, loss = 0.43553538\n",
            "Iteration 134, loss = 0.43477046\n",
            "Iteration 135, loss = 0.43401003\n",
            "Iteration 136, loss = 0.43328200\n",
            "Iteration 137, loss = 0.43256891\n",
            "Iteration 138, loss = 0.43188468\n",
            "Iteration 139, loss = 0.43115973\n",
            "Iteration 140, loss = 0.43048931\n",
            "Iteration 141, loss = 0.42982869\n",
            "Iteration 142, loss = 0.42916293\n",
            "Iteration 143, loss = 0.42849261\n",
            "Iteration 144, loss = 0.42785440\n",
            "Iteration 145, loss = 0.42718858\n",
            "Iteration 146, loss = 0.42654208\n",
            "Iteration 147, loss = 0.42591418\n",
            "Iteration 148, loss = 0.42530458\n",
            "Iteration 149, loss = 0.42466028\n",
            "Iteration 150, loss = 0.42404951\n",
            "Iteration 151, loss = 0.42346106\n",
            "Iteration 152, loss = 0.42285286\n",
            "Iteration 153, loss = 0.42224729\n",
            "Iteration 154, loss = 0.42167292\n",
            "Iteration 155, loss = 0.42108779\n",
            "Iteration 156, loss = 0.42052741\n",
            "Iteration 157, loss = 0.41995887\n",
            "Iteration 158, loss = 0.41941636\n",
            "Iteration 159, loss = 0.41887242\n",
            "Iteration 160, loss = 0.41834747\n",
            "Iteration 161, loss = 0.41781151\n",
            "Iteration 162, loss = 0.41729784\n",
            "Iteration 163, loss = 0.41677983\n",
            "Iteration 164, loss = 0.41626024\n",
            "Iteration 165, loss = 0.41576094\n",
            "Iteration 166, loss = 0.41527334\n",
            "Iteration 167, loss = 0.41478447\n",
            "Iteration 168, loss = 0.41430398\n",
            "Iteration 169, loss = 0.41381413\n",
            "Iteration 170, loss = 0.41334716\n",
            "Iteration 171, loss = 0.41287120\n",
            "Iteration 172, loss = 0.41241218\n",
            "Iteration 173, loss = 0.41196433\n",
            "Iteration 174, loss = 0.41149022\n",
            "Iteration 175, loss = 0.41104803\n",
            "Iteration 176, loss = 0.41059880\n",
            "Iteration 177, loss = 0.41015154\n",
            "Iteration 178, loss = 0.40970798\n",
            "Iteration 179, loss = 0.40928383\n",
            "Iteration 180, loss = 0.40883614\n",
            "Iteration 181, loss = 0.40841013\n",
            "Iteration 182, loss = 0.40796609\n",
            "Iteration 183, loss = 0.40754655\n",
            "Iteration 184, loss = 0.40710725\n",
            "Iteration 185, loss = 0.40667361\n",
            "Iteration 186, loss = 0.40626612\n",
            "Iteration 187, loss = 0.40582821\n",
            "Iteration 188, loss = 0.40542523\n",
            "Iteration 189, loss = 0.40498826\n",
            "Iteration 190, loss = 0.40457914\n",
            "Iteration 191, loss = 0.40415815\n",
            "Iteration 192, loss = 0.40374534\n",
            "Iteration 193, loss = 0.40331979\n",
            "Iteration 194, loss = 0.40292238\n",
            "Iteration 195, loss = 0.40247945\n",
            "Iteration 196, loss = 0.40206694\n",
            "Iteration 197, loss = 0.40166278\n",
            "Iteration 198, loss = 0.40124127\n",
            "Iteration 199, loss = 0.40084317\n",
            "Iteration 200, loss = 0.40043731\n",
            "Iteration 201, loss = 0.40005165\n",
            "Iteration 202, loss = 0.39965850\n",
            "Iteration 203, loss = 0.39926925\n",
            "Iteration 204, loss = 0.39888457\n",
            "Iteration 205, loss = 0.39848366\n",
            "Iteration 206, loss = 0.39809998\n",
            "Iteration 207, loss = 0.39769468\n",
            "Iteration 208, loss = 0.39729553\n",
            "Iteration 209, loss = 0.39689206\n",
            "Iteration 210, loss = 0.39649039\n",
            "Iteration 211, loss = 0.39609414\n",
            "Iteration 212, loss = 0.39569826\n",
            "Iteration 213, loss = 0.39529745\n",
            "Iteration 214, loss = 0.39489582\n",
            "Iteration 215, loss = 0.39450327\n",
            "Iteration 216, loss = 0.39410520\n",
            "Iteration 217, loss = 0.39370512\n",
            "Iteration 218, loss = 0.39328715\n",
            "Iteration 219, loss = 0.39289904\n",
            "Iteration 220, loss = 0.39249843\n",
            "Iteration 221, loss = 0.39211364\n",
            "Iteration 222, loss = 0.39172864\n",
            "Iteration 223, loss = 0.39135277\n",
            "Iteration 224, loss = 0.39097936\n",
            "Iteration 225, loss = 0.39059732\n",
            "Iteration 226, loss = 0.39023044\n",
            "Iteration 227, loss = 0.38987282\n",
            "Iteration 228, loss = 0.38949717\n",
            "Iteration 229, loss = 0.38913540\n",
            "Iteration 230, loss = 0.38879746\n",
            "Iteration 231, loss = 0.38842611\n",
            "Iteration 232, loss = 0.38807619\n",
            "Iteration 233, loss = 0.38772604\n",
            "Iteration 234, loss = 0.38737727\n",
            "Iteration 235, loss = 0.38703709\n",
            "Iteration 236, loss = 0.38668342\n",
            "Iteration 237, loss = 0.38634484\n",
            "Iteration 238, loss = 0.38600345\n",
            "Iteration 239, loss = 0.38564272\n",
            "Iteration 240, loss = 0.38528545\n",
            "Iteration 241, loss = 0.38493249\n",
            "Iteration 242, loss = 0.38457739\n",
            "Iteration 243, loss = 0.38420483\n",
            "Iteration 244, loss = 0.38384460\n",
            "Iteration 245, loss = 0.38347483\n",
            "Iteration 246, loss = 0.38310736\n",
            "Iteration 247, loss = 0.38274367\n",
            "Iteration 248, loss = 0.38239808\n",
            "Iteration 249, loss = 0.38201550\n",
            "Iteration 250, loss = 0.38164285\n",
            "Iteration 251, loss = 0.38127021\n",
            "Iteration 252, loss = 0.38089918\n",
            "Iteration 253, loss = 0.38051695\n",
            "Iteration 254, loss = 0.38013164\n",
            "Iteration 255, loss = 0.37973934\n",
            "Iteration 256, loss = 0.37933547\n",
            "Iteration 257, loss = 0.37894229\n",
            "Iteration 258, loss = 0.37854033\n",
            "Iteration 259, loss = 0.37810013\n",
            "Iteration 260, loss = 0.37768418\n",
            "Iteration 261, loss = 0.37726185\n",
            "Iteration 262, loss = 0.37685173\n",
            "Iteration 263, loss = 0.37643809\n",
            "Iteration 264, loss = 0.37603711\n",
            "Iteration 265, loss = 0.37560035\n",
            "Iteration 266, loss = 0.37515368\n",
            "Iteration 267, loss = 0.37472558\n",
            "Iteration 268, loss = 0.37427266\n",
            "Iteration 269, loss = 0.37383443\n",
            "Iteration 270, loss = 0.37340160\n",
            "Iteration 271, loss = 0.37296570\n",
            "Iteration 272, loss = 0.37252067\n",
            "Iteration 273, loss = 0.37209153\n",
            "Iteration 274, loss = 0.37164152\n",
            "Iteration 275, loss = 0.37118806\n",
            "Iteration 276, loss = 0.37074910\n",
            "Iteration 277, loss = 0.37031183\n",
            "Iteration 278, loss = 0.36986562\n",
            "Iteration 279, loss = 0.36941087\n",
            "Iteration 280, loss = 0.36898801\n",
            "Iteration 281, loss = 0.36852416\n",
            "Iteration 282, loss = 0.36806736\n",
            "Iteration 283, loss = 0.36758966\n",
            "Iteration 284, loss = 0.36711642\n",
            "Iteration 285, loss = 0.36664305\n",
            "Iteration 286, loss = 0.36616461\n",
            "Iteration 287, loss = 0.36568561\n",
            "Iteration 288, loss = 0.36518453\n",
            "Iteration 289, loss = 0.36469517\n",
            "Iteration 290, loss = 0.36417865\n",
            "Iteration 291, loss = 0.36365475\n",
            "Iteration 292, loss = 0.36312480\n",
            "Iteration 293, loss = 0.36258220\n",
            "Iteration 294, loss = 0.36203666\n",
            "Iteration 295, loss = 0.36147313\n",
            "Iteration 296, loss = 0.36094078\n",
            "Iteration 297, loss = 0.36035644\n",
            "Iteration 298, loss = 0.35982584\n",
            "Iteration 299, loss = 0.35926046\n",
            "Iteration 300, loss = 0.35870456\n",
            "Iteration 301, loss = 0.35811794\n",
            "Iteration 302, loss = 0.35751187\n",
            "Iteration 303, loss = 0.35691245\n",
            "Iteration 304, loss = 0.35630987\n",
            "Iteration 305, loss = 0.35567942\n",
            "Iteration 306, loss = 0.35503785\n",
            "Iteration 307, loss = 0.35443796\n",
            "Iteration 308, loss = 0.35376882\n",
            "Iteration 309, loss = 0.35311255\n",
            "Iteration 310, loss = 0.35244642\n",
            "Iteration 311, loss = 0.35175228\n",
            "Iteration 312, loss = 0.35104411\n",
            "Iteration 313, loss = 0.35025294\n",
            "Iteration 314, loss = 0.34940078\n",
            "Iteration 315, loss = 0.34849131\n",
            "Iteration 316, loss = 0.34764898\n",
            "Iteration 317, loss = 0.34676432\n",
            "Iteration 318, loss = 0.34591202\n",
            "Iteration 319, loss = 0.34499077\n",
            "Iteration 320, loss = 0.34401970\n",
            "Iteration 321, loss = 0.34303744\n",
            "Iteration 322, loss = 0.34194535\n",
            "Iteration 323, loss = 0.34091084\n",
            "Iteration 324, loss = 0.33987462\n",
            "Iteration 325, loss = 0.33889090\n",
            "Iteration 326, loss = 0.33784602\n",
            "Iteration 327, loss = 0.33678014\n",
            "Iteration 328, loss = 0.33563505\n",
            "Iteration 329, loss = 0.33461122\n",
            "Iteration 330, loss = 0.33361008\n",
            "Iteration 331, loss = 0.33249432\n",
            "Iteration 332, loss = 0.33136090\n",
            "Iteration 333, loss = 0.33019729\n",
            "Iteration 334, loss = 0.32905674\n",
            "Iteration 335, loss = 0.32780096\n",
            "Iteration 336, loss = 0.32651471\n",
            "Iteration 337, loss = 0.32522320\n",
            "Iteration 338, loss = 0.32386480\n",
            "Iteration 339, loss = 0.32244708\n",
            "Iteration 340, loss = 0.32093150\n",
            "Iteration 341, loss = 0.31952274\n",
            "Iteration 342, loss = 0.31783602\n",
            "Iteration 343, loss = 0.31615188\n",
            "Iteration 344, loss = 0.31444763\n",
            "Iteration 345, loss = 0.31258319\n",
            "Iteration 346, loss = 0.31089076\n",
            "Iteration 347, loss = 0.30918213\n",
            "Iteration 348, loss = 0.30753292\n",
            "Iteration 349, loss = 0.30590081\n",
            "Iteration 350, loss = 0.30418424\n",
            "Iteration 351, loss = 0.30245770\n",
            "Iteration 352, loss = 0.30065592\n",
            "Iteration 353, loss = 0.29851053\n",
            "Iteration 354, loss = 0.29636564\n",
            "Iteration 355, loss = 0.29420623\n",
            "Iteration 356, loss = 0.29206119\n",
            "Iteration 357, loss = 0.29001875\n",
            "Iteration 358, loss = 0.28788837\n",
            "Iteration 359, loss = 0.28574974\n",
            "Iteration 360, loss = 0.28361283\n",
            "Iteration 361, loss = 0.28161178\n",
            "Iteration 362, loss = 0.27950941\n",
            "Iteration 363, loss = 0.27755854\n",
            "Iteration 364, loss = 0.27572386\n",
            "Iteration 365, loss = 0.27406583\n",
            "Iteration 366, loss = 0.27232465\n",
            "Iteration 367, loss = 0.27052946\n",
            "Iteration 368, loss = 0.26907482\n",
            "Iteration 369, loss = 0.26732689\n",
            "Iteration 370, loss = 0.26571766\n",
            "Iteration 371, loss = 0.26413350\n",
            "Iteration 372, loss = 0.26259011\n",
            "Iteration 373, loss = 0.26110039\n",
            "Iteration 374, loss = 0.25969233\n",
            "Iteration 375, loss = 0.25821525\n",
            "Iteration 376, loss = 0.25690722\n",
            "Iteration 377, loss = 0.25551015\n",
            "Iteration 378, loss = 0.25418795\n",
            "Iteration 379, loss = 0.25283890\n",
            "Iteration 380, loss = 0.25156206\n",
            "Iteration 381, loss = 0.25031822\n",
            "Iteration 382, loss = 0.24901560\n",
            "Iteration 383, loss = 0.24781443\n",
            "Iteration 384, loss = 0.24655003\n",
            "Iteration 385, loss = 0.24535484\n",
            "Iteration 386, loss = 0.24414324\n",
            "Iteration 387, loss = 0.24298176\n",
            "Iteration 388, loss = 0.24175541\n",
            "Iteration 389, loss = 0.24072950\n",
            "Iteration 390, loss = 0.23957776\n",
            "Iteration 391, loss = 0.23854798\n",
            "Iteration 392, loss = 0.23754229\n",
            "Iteration 393, loss = 0.23649344\n",
            "Iteration 394, loss = 0.23550562\n",
            "Iteration 395, loss = 0.23451199\n",
            "Iteration 396, loss = 0.23354547\n",
            "Iteration 397, loss = 0.23258629\n",
            "Iteration 398, loss = 0.23163190\n",
            "Iteration 399, loss = 0.23072610\n",
            "Iteration 400, loss = 0.22981908\n",
            "Iteration 401, loss = 0.22895667\n",
            "Iteration 402, loss = 0.22803382\n",
            "Iteration 403, loss = 0.22721630\n",
            "Iteration 404, loss = 0.22631185\n",
            "Iteration 405, loss = 0.22545606\n",
            "Iteration 406, loss = 0.22458173\n",
            "Iteration 407, loss = 0.22370437\n",
            "Iteration 408, loss = 0.22292498\n",
            "Iteration 409, loss = 0.22209444\n",
            "Iteration 410, loss = 0.22130354\n",
            "Iteration 411, loss = 0.22044590\n",
            "Iteration 412, loss = 0.21963714\n",
            "Iteration 413, loss = 0.21888064\n",
            "Iteration 414, loss = 0.21810257\n",
            "Iteration 415, loss = 0.21743551\n",
            "Iteration 416, loss = 0.21656385\n",
            "Iteration 417, loss = 0.21587448\n",
            "Iteration 418, loss = 0.21507164\n",
            "Iteration 419, loss = 0.21437135\n",
            "Iteration 420, loss = 0.21365226\n",
            "Iteration 421, loss = 0.21299809\n",
            "Iteration 422, loss = 0.21217908\n",
            "Iteration 423, loss = 0.21151556\n",
            "Iteration 424, loss = 0.21081655\n",
            "Iteration 425, loss = 0.21018382\n",
            "Iteration 426, loss = 0.20950627\n",
            "Iteration 427, loss = 0.20880854\n",
            "Iteration 428, loss = 0.20821014\n",
            "Iteration 429, loss = 0.20744635\n",
            "Iteration 430, loss = 0.20688789\n",
            "Iteration 431, loss = 0.20617647\n",
            "Iteration 432, loss = 0.20555173\n",
            "Iteration 433, loss = 0.20503428\n",
            "Iteration 434, loss = 0.20443448\n",
            "Iteration 435, loss = 0.20379671\n",
            "Iteration 436, loss = 0.20320385\n",
            "Iteration 437, loss = 0.20261191\n",
            "Iteration 438, loss = 0.20206036\n",
            "Iteration 439, loss = 0.20153367\n",
            "Iteration 440, loss = 0.20099080\n",
            "Iteration 441, loss = 0.20042623\n",
            "Iteration 442, loss = 0.19984121\n",
            "Iteration 443, loss = 0.19933606\n",
            "Iteration 444, loss = 0.19879776\n",
            "Iteration 445, loss = 0.19824771\n",
            "Iteration 446, loss = 0.19786182\n",
            "Iteration 447, loss = 0.19728248\n",
            "Iteration 448, loss = 0.19677415\n",
            "Iteration 449, loss = 0.19622496\n",
            "Iteration 450, loss = 0.19580217\n",
            "Iteration 451, loss = 0.19526343\n",
            "Iteration 452, loss = 0.19474860\n",
            "Iteration 453, loss = 0.19428585\n",
            "Iteration 454, loss = 0.19381736\n",
            "Iteration 455, loss = 0.19331556\n",
            "Iteration 456, loss = 0.19282631\n",
            "Iteration 457, loss = 0.19238167\n",
            "Iteration 458, loss = 0.19191867\n",
            "Iteration 459, loss = 0.19145100\n",
            "Iteration 460, loss = 0.19095277\n",
            "Iteration 461, loss = 0.19050310\n",
            "Iteration 462, loss = 0.19007461\n",
            "Iteration 463, loss = 0.18970896\n",
            "Iteration 464, loss = 0.18918255\n",
            "Iteration 465, loss = 0.18871322\n",
            "Iteration 466, loss = 0.18823548\n",
            "Iteration 467, loss = 0.18797201\n",
            "Iteration 468, loss = 0.18744433\n",
            "Iteration 469, loss = 0.18694841\n",
            "Iteration 470, loss = 0.18655955\n",
            "Iteration 471, loss = 0.18618286\n",
            "Iteration 472, loss = 0.18568272\n",
            "Iteration 473, loss = 0.18536432\n",
            "Iteration 474, loss = 0.18493300\n",
            "Iteration 475, loss = 0.18446652\n",
            "Iteration 476, loss = 0.18407105\n",
            "Iteration 477, loss = 0.18364978\n",
            "Iteration 478, loss = 0.18324504\n",
            "Iteration 479, loss = 0.18282313\n",
            "Iteration 480, loss = 0.18244098\n",
            "Iteration 481, loss = 0.18207374\n",
            "Iteration 482, loss = 0.18166243\n",
            "Iteration 483, loss = 0.18130924\n",
            "Iteration 484, loss = 0.18096612\n",
            "Iteration 485, loss = 0.18050663\n",
            "Iteration 486, loss = 0.18016056\n",
            "Iteration 487, loss = 0.17979625\n",
            "Iteration 488, loss = 0.17939635\n",
            "Iteration 489, loss = 0.17899127\n",
            "Iteration 490, loss = 0.17860310\n",
            "Iteration 491, loss = 0.17834307\n",
            "Iteration 492, loss = 0.17795637\n",
            "Iteration 493, loss = 0.17754462\n",
            "Iteration 494, loss = 0.17720774\n",
            "Iteration 495, loss = 0.17686297\n",
            "Iteration 496, loss = 0.17655920\n",
            "Iteration 497, loss = 0.17621150\n",
            "Iteration 498, loss = 0.17587486\n",
            "Iteration 499, loss = 0.17551515\n",
            "Iteration 500, loss = 0.17522677\n",
            "Iteration 501, loss = 0.17484126\n",
            "Iteration 502, loss = 0.17460024\n",
            "Iteration 503, loss = 0.17414052\n",
            "Iteration 504, loss = 0.17383233\n",
            "Iteration 505, loss = 0.17353377\n",
            "Iteration 506, loss = 0.17315497\n",
            "Iteration 507, loss = 0.17298865\n",
            "Iteration 508, loss = 0.17257057\n",
            "Iteration 509, loss = 0.17226555\n",
            "Iteration 510, loss = 0.17195207\n",
            "Iteration 511, loss = 0.17171367\n",
            "Iteration 512, loss = 0.17132959\n",
            "Iteration 513, loss = 0.17104996\n",
            "Iteration 514, loss = 0.17067569\n",
            "Iteration 515, loss = 0.17041125\n",
            "Iteration 516, loss = 0.17014243\n",
            "Iteration 517, loss = 0.16981143\n",
            "Iteration 518, loss = 0.16960049\n",
            "Iteration 519, loss = 0.16920319\n",
            "Iteration 520, loss = 0.16891104\n",
            "Iteration 521, loss = 0.16863698\n",
            "Iteration 522, loss = 0.16836624\n",
            "Iteration 523, loss = 0.16811683\n",
            "Iteration 524, loss = 0.16777706\n",
            "Iteration 525, loss = 0.16755080\n",
            "Iteration 526, loss = 0.16724084\n",
            "Iteration 527, loss = 0.16699177\n",
            "Iteration 528, loss = 0.16675025\n",
            "Iteration 529, loss = 0.16644166\n",
            "Iteration 530, loss = 0.16616882\n",
            "Iteration 531, loss = 0.16596597\n",
            "Iteration 532, loss = 0.16563780\n",
            "Iteration 533, loss = 0.16535642\n",
            "Iteration 534, loss = 0.16515698\n",
            "Iteration 535, loss = 0.16490583\n",
            "Iteration 536, loss = 0.16460489\n",
            "Iteration 537, loss = 0.16435522\n",
            "Iteration 538, loss = 0.16411326\n",
            "Iteration 539, loss = 0.16380512\n",
            "Iteration 540, loss = 0.16357727\n",
            "Iteration 541, loss = 0.16331884\n",
            "Iteration 542, loss = 0.16306399\n",
            "Iteration 543, loss = 0.16280490\n",
            "Iteration 544, loss = 0.16256442\n",
            "Iteration 545, loss = 0.16228435\n",
            "Iteration 546, loss = 0.16204569\n",
            "Iteration 547, loss = 0.16182258\n",
            "Iteration 548, loss = 0.16149401\n",
            "Iteration 549, loss = 0.16122820\n",
            "Iteration 550, loss = 0.16093579\n",
            "Iteration 551, loss = 0.16072238\n",
            "Iteration 552, loss = 0.16041674\n",
            "Iteration 553, loss = 0.16023056\n",
            "Iteration 554, loss = 0.15993878\n",
            "Iteration 555, loss = 0.15968823\n",
            "Iteration 556, loss = 0.15947772\n",
            "Iteration 557, loss = 0.15929853\n",
            "Iteration 558, loss = 0.15910118\n",
            "Iteration 559, loss = 0.15886128\n",
            "Iteration 560, loss = 0.15846878\n",
            "Iteration 561, loss = 0.15826409\n",
            "Iteration 562, loss = 0.15809419\n",
            "Iteration 563, loss = 0.15788245\n",
            "Iteration 564, loss = 0.15756378\n",
            "Iteration 565, loss = 0.15745648\n",
            "Iteration 566, loss = 0.15717421\n",
            "Iteration 567, loss = 0.15691332\n",
            "Iteration 568, loss = 0.15667206\n",
            "Iteration 569, loss = 0.15645015\n",
            "Iteration 570, loss = 0.15630247\n",
            "Iteration 571, loss = 0.15602261\n",
            "Iteration 572, loss = 0.15586637\n",
            "Iteration 573, loss = 0.15556971\n",
            "Iteration 574, loss = 0.15544105\n",
            "Iteration 575, loss = 0.15527339\n",
            "Iteration 576, loss = 0.15493864\n",
            "Iteration 577, loss = 0.15472989\n",
            "Iteration 578, loss = 0.15455836\n",
            "Iteration 579, loss = 0.15439541\n",
            "Iteration 580, loss = 0.15414240\n",
            "Iteration 581, loss = 0.15406685\n",
            "Iteration 582, loss = 0.15377345\n",
            "Iteration 583, loss = 0.15360509\n",
            "Iteration 584, loss = 0.15338140\n",
            "Iteration 585, loss = 0.15322951\n",
            "Iteration 586, loss = 0.15299335\n",
            "Iteration 587, loss = 0.15279049\n",
            "Iteration 588, loss = 0.15254785\n",
            "Iteration 589, loss = 0.15249667\n",
            "Iteration 590, loss = 0.15222398\n",
            "Iteration 591, loss = 0.15203604\n",
            "Iteration 592, loss = 0.15186815\n",
            "Iteration 593, loss = 0.15173474\n",
            "Iteration 594, loss = 0.15152674\n",
            "Iteration 595, loss = 0.15131126\n",
            "Iteration 596, loss = 0.15110310\n",
            "Iteration 597, loss = 0.15097834\n",
            "Iteration 598, loss = 0.15080709\n",
            "Iteration 599, loss = 0.15064137\n",
            "Iteration 600, loss = 0.15045995\n",
            "Iteration 601, loss = 0.15023081\n",
            "Iteration 602, loss = 0.15018006\n",
            "Iteration 603, loss = 0.14987433\n",
            "Iteration 604, loss = 0.14970275\n",
            "Iteration 605, loss = 0.14961288\n",
            "Iteration 606, loss = 0.14937482\n",
            "Iteration 607, loss = 0.14918324\n",
            "Iteration 608, loss = 0.14902171\n",
            "Iteration 609, loss = 0.14888435\n",
            "Iteration 610, loss = 0.14869354\n",
            "Iteration 611, loss = 0.14855853\n",
            "Iteration 612, loss = 0.14837708\n",
            "Iteration 613, loss = 0.14820534\n",
            "Iteration 614, loss = 0.14807648\n",
            "Iteration 615, loss = 0.14795326\n",
            "Iteration 616, loss = 0.14775572\n",
            "Iteration 617, loss = 0.14767835\n",
            "Iteration 618, loss = 0.14742329\n",
            "Iteration 619, loss = 0.14724054\n",
            "Iteration 620, loss = 0.14708692\n",
            "Iteration 621, loss = 0.14712752\n",
            "Iteration 622, loss = 0.14683154\n",
            "Iteration 623, loss = 0.14665978\n",
            "Iteration 624, loss = 0.14647102\n",
            "Iteration 625, loss = 0.14631251\n",
            "Iteration 626, loss = 0.14620684\n",
            "Iteration 627, loss = 0.14613920\n",
            "Iteration 628, loss = 0.14588984\n",
            "Iteration 629, loss = 0.14571793\n",
            "Iteration 630, loss = 0.14554601\n",
            "Iteration 631, loss = 0.14544026\n",
            "Iteration 632, loss = 0.14542829\n",
            "Iteration 633, loss = 0.14516681\n",
            "Iteration 634, loss = 0.14497077\n",
            "Iteration 635, loss = 0.14485928\n",
            "Iteration 636, loss = 0.14473786\n",
            "Iteration 637, loss = 0.14467426\n",
            "Iteration 638, loss = 0.14442181\n",
            "Iteration 639, loss = 0.14426351\n",
            "Iteration 640, loss = 0.14416920\n",
            "Iteration 641, loss = 0.14405059\n",
            "Iteration 642, loss = 0.14393945\n",
            "Iteration 643, loss = 0.14376117\n",
            "Iteration 644, loss = 0.14359927\n",
            "Iteration 645, loss = 0.14345728\n",
            "Iteration 646, loss = 0.14330182\n",
            "Iteration 647, loss = 0.14325253\n",
            "Iteration 648, loss = 0.14308553\n",
            "Iteration 649, loss = 0.14291499\n",
            "Iteration 650, loss = 0.14277563\n",
            "Iteration 651, loss = 0.14263321\n",
            "Iteration 652, loss = 0.14251089\n",
            "Iteration 653, loss = 0.14240397\n",
            "Iteration 654, loss = 0.14233972\n",
            "Iteration 655, loss = 0.14218576\n",
            "Iteration 656, loss = 0.14205018\n",
            "Iteration 657, loss = 0.14193062\n",
            "Iteration 658, loss = 0.14178646\n",
            "Iteration 659, loss = 0.14166567\n",
            "Iteration 660, loss = 0.14151414\n",
            "Iteration 661, loss = 0.14136179\n",
            "Iteration 662, loss = 0.14124162\n",
            "Iteration 663, loss = 0.14117878\n",
            "Iteration 664, loss = 0.14110164\n",
            "Iteration 665, loss = 0.14092051\n",
            "Iteration 666, loss = 0.14084949\n",
            "Iteration 667, loss = 0.14065973\n",
            "Iteration 668, loss = 0.14055571\n",
            "Iteration 669, loss = 0.14045294\n",
            "Iteration 670, loss = 0.14040743\n",
            "Iteration 671, loss = 0.14028638\n",
            "Iteration 672, loss = 0.14010899\n",
            "Iteration 673, loss = 0.13995657\n",
            "Iteration 674, loss = 0.13990817\n",
            "Iteration 675, loss = 0.13983972\n",
            "Iteration 676, loss = 0.13958816\n",
            "Iteration 677, loss = 0.13949979\n",
            "Iteration 678, loss = 0.13939749\n",
            "Iteration 679, loss = 0.13939369\n",
            "Iteration 680, loss = 0.13916380\n",
            "Iteration 681, loss = 0.13906167\n",
            "Iteration 682, loss = 0.13895802\n",
            "Iteration 683, loss = 0.13885432\n",
            "Iteration 684, loss = 0.13875820\n",
            "Iteration 685, loss = 0.13864351\n",
            "Iteration 686, loss = 0.13852490\n",
            "Iteration 687, loss = 0.13841310\n",
            "Iteration 688, loss = 0.13858554\n",
            "Iteration 689, loss = 0.13822293\n",
            "Iteration 690, loss = 0.13811996\n",
            "Iteration 691, loss = 0.13796822\n",
            "Iteration 692, loss = 0.13791838\n",
            "Iteration 693, loss = 0.13774504\n",
            "Iteration 694, loss = 0.13769829\n",
            "Iteration 695, loss = 0.13767761\n",
            "Iteration 696, loss = 0.13750255\n",
            "Iteration 697, loss = 0.13742882\n",
            "Iteration 698, loss = 0.13726386\n",
            "Iteration 699, loss = 0.13718231\n",
            "Iteration 700, loss = 0.13708980\n",
            "Iteration 701, loss = 0.13696479\n",
            "Iteration 702, loss = 0.13690159\n",
            "Iteration 703, loss = 0.13686017\n",
            "Iteration 704, loss = 0.13666134\n",
            "Iteration 705, loss = 0.13665152\n",
            "Iteration 706, loss = 0.13647982\n",
            "Iteration 707, loss = 0.13650805\n",
            "Iteration 708, loss = 0.13629591\n",
            "Iteration 709, loss = 0.13624576\n",
            "Iteration 710, loss = 0.13612599\n",
            "Iteration 711, loss = 0.13602278\n",
            "Iteration 712, loss = 0.13590633\n",
            "Iteration 713, loss = 0.13580808\n",
            "Iteration 714, loss = 0.13571603\n",
            "Iteration 715, loss = 0.13558130\n",
            "Iteration 716, loss = 0.13551590\n",
            "Iteration 717, loss = 0.13549407\n",
            "Iteration 718, loss = 0.13537930\n",
            "Iteration 719, loss = 0.13528063\n",
            "Iteration 720, loss = 0.13515491\n",
            "Iteration 721, loss = 0.13506066\n",
            "Iteration 722, loss = 0.13493852\n",
            "Iteration 723, loss = 0.13492375\n",
            "Iteration 724, loss = 0.13489291\n",
            "Iteration 725, loss = 0.13482438\n",
            "Iteration 726, loss = 0.13464053\n",
            "Iteration 727, loss = 0.13452493\n",
            "Iteration 728, loss = 0.13450561\n",
            "Iteration 729, loss = 0.13437888\n",
            "Iteration 730, loss = 0.13427570\n",
            "Iteration 731, loss = 0.13415114\n",
            "Iteration 732, loss = 0.13410165\n",
            "Iteration 733, loss = 0.13407875\n",
            "Iteration 734, loss = 0.13402317\n",
            "Iteration 735, loss = 0.13391975\n",
            "Iteration 736, loss = 0.13380090\n",
            "Iteration 737, loss = 0.13367872\n",
            "Iteration 738, loss = 0.13357332\n",
            "Iteration 739, loss = 0.13362405\n",
            "Iteration 740, loss = 0.13341080\n",
            "Iteration 741, loss = 0.13332415\n",
            "Iteration 742, loss = 0.13324510\n",
            "Iteration 743, loss = 0.13315194\n",
            "Iteration 744, loss = 0.13306560\n",
            "Iteration 745, loss = 0.13297389\n",
            "Iteration 746, loss = 0.13291833\n",
            "Iteration 747, loss = 0.13287582\n",
            "Iteration 748, loss = 0.13273759\n",
            "Iteration 749, loss = 0.13267089\n",
            "Iteration 750, loss = 0.13258495\n",
            "Iteration 751, loss = 0.13250846\n",
            "Iteration 752, loss = 0.13242786\n",
            "Iteration 753, loss = 0.13236288\n",
            "Iteration 754, loss = 0.13241793\n",
            "Iteration 755, loss = 0.13220978\n",
            "Iteration 756, loss = 0.13212818\n",
            "Iteration 757, loss = 0.13203155\n",
            "Iteration 758, loss = 0.13201486\n",
            "Iteration 759, loss = 0.13187599\n",
            "Iteration 760, loss = 0.13185740\n",
            "Iteration 761, loss = 0.13175994\n",
            "Iteration 762, loss = 0.13165156\n",
            "Iteration 763, loss = 0.13156917\n",
            "Iteration 764, loss = 0.13147440\n",
            "Iteration 765, loss = 0.13141876\n",
            "Iteration 766, loss = 0.13138744\n",
            "Iteration 767, loss = 0.13127402\n",
            "Iteration 768, loss = 0.13120611\n",
            "Iteration 769, loss = 0.13116070\n",
            "Iteration 770, loss = 0.13113164\n",
            "Iteration 771, loss = 0.13107021\n",
            "Iteration 772, loss = 0.13093613\n",
            "Iteration 773, loss = 0.13089577\n",
            "Iteration 774, loss = 0.13084646\n",
            "Iteration 775, loss = 0.13071040\n",
            "Iteration 776, loss = 0.13075752\n",
            "Iteration 777, loss = 0.13070801\n",
            "Iteration 778, loss = 0.13053710\n",
            "Iteration 779, loss = 0.13046161\n",
            "Iteration 780, loss = 0.13034512\n",
            "Iteration 781, loss = 0.13031333\n",
            "Iteration 782, loss = 0.13022642\n",
            "Iteration 783, loss = 0.13017239\n",
            "Iteration 784, loss = 0.13012425\n",
            "Iteration 785, loss = 0.13003988\n",
            "Iteration 786, loss = 0.12997900\n",
            "Iteration 787, loss = 0.12988792\n",
            "Iteration 788, loss = 0.12983779\n",
            "Iteration 789, loss = 0.12974765\n",
            "Iteration 790, loss = 0.12969084\n",
            "Iteration 791, loss = 0.12961927\n",
            "Iteration 792, loss = 0.12955216\n",
            "Iteration 793, loss = 0.12947298\n",
            "Iteration 794, loss = 0.12939192\n",
            "Iteration 795, loss = 0.12937794\n",
            "Iteration 796, loss = 0.12937866\n",
            "Iteration 797, loss = 0.12931983\n",
            "Iteration 798, loss = 0.12917649\n",
            "Iteration 799, loss = 0.12913302\n",
            "Iteration 800, loss = 0.12909360\n",
            "Iteration 801, loss = 0.12902876\n",
            "Iteration 802, loss = 0.12891262\n",
            "Iteration 803, loss = 0.12892782\n",
            "Iteration 804, loss = 0.12884035\n",
            "Iteration 805, loss = 0.12870392\n",
            "Iteration 806, loss = 0.12869016\n",
            "Iteration 807, loss = 0.12858679\n",
            "Iteration 808, loss = 0.12858744\n",
            "Iteration 809, loss = 0.12850307\n",
            "Iteration 810, loss = 0.12844159\n",
            "Iteration 811, loss = 0.12845302\n",
            "Iteration 812, loss = 0.12830947\n",
            "Iteration 813, loss = 0.12834403\n",
            "Iteration 814, loss = 0.12831108\n",
            "Iteration 815, loss = 0.12817381\n",
            "Iteration 816, loss = 0.12805479\n",
            "Iteration 817, loss = 0.12806681\n",
            "Iteration 818, loss = 0.12795056\n",
            "Iteration 819, loss = 0.12794786\n",
            "Iteration 820, loss = 0.12792026\n",
            "Iteration 821, loss = 0.12780186\n",
            "Iteration 822, loss = 0.12779110\n",
            "Iteration 823, loss = 0.12785674\n",
            "Iteration 824, loss = 0.12765333\n",
            "Iteration 825, loss = 0.12761360\n",
            "Iteration 826, loss = 0.12761804\n",
            "Iteration 827, loss = 0.12747201\n",
            "Iteration 828, loss = 0.12743212\n",
            "Iteration 829, loss = 0.12733320\n",
            "Iteration 830, loss = 0.12727563\n",
            "Iteration 831, loss = 0.12722334\n",
            "Iteration 832, loss = 0.12723506\n",
            "Iteration 833, loss = 0.12711228\n",
            "Iteration 834, loss = 0.12706964\n",
            "Iteration 835, loss = 0.12701148\n",
            "Iteration 836, loss = 0.12695556\n",
            "Iteration 837, loss = 0.12689847\n",
            "Iteration 838, loss = 0.12693429\n",
            "Iteration 839, loss = 0.12683085\n",
            "Iteration 840, loss = 0.12676178\n",
            "Iteration 841, loss = 0.12669638\n",
            "Iteration 842, loss = 0.12664708\n",
            "Iteration 843, loss = 0.12656549\n",
            "Iteration 844, loss = 0.12654039\n",
            "Iteration 845, loss = 0.12671576\n",
            "Iteration 846, loss = 0.12648393\n",
            "Iteration 847, loss = 0.12643953\n",
            "Iteration 848, loss = 0.12636840\n",
            "Iteration 849, loss = 0.12625799\n",
            "Iteration 850, loss = 0.12621401\n",
            "Iteration 851, loss = 0.12623741\n",
            "Iteration 852, loss = 0.12617109\n",
            "Iteration 853, loss = 0.12612461\n",
            "Iteration 854, loss = 0.12604489\n",
            "Iteration 855, loss = 0.12602516\n",
            "Iteration 856, loss = 0.12593030\n",
            "Iteration 857, loss = 0.12588271\n",
            "Iteration 858, loss = 0.12584031\n",
            "Iteration 859, loss = 0.12584541\n",
            "Iteration 860, loss = 0.12572918\n",
            "Iteration 861, loss = 0.12571283\n",
            "Iteration 862, loss = 0.12570887\n",
            "Iteration 863, loss = 0.12564972\n",
            "Iteration 864, loss = 0.12556409\n",
            "Iteration 865, loss = 0.12560811\n",
            "Iteration 866, loss = 0.12545358\n",
            "Iteration 867, loss = 0.12539690\n",
            "Iteration 868, loss = 0.12548799\n",
            "Iteration 869, loss = 0.12550985\n",
            "Iteration 870, loss = 0.12531067\n",
            "Iteration 871, loss = 0.12526606\n",
            "Iteration 872, loss = 0.12515913\n",
            "Iteration 873, loss = 0.12515964\n",
            "Iteration 874, loss = 0.12518359\n",
            "Iteration 875, loss = 0.12514131\n",
            "Iteration 876, loss = 0.12502004\n",
            "Iteration 877, loss = 0.12512449\n",
            "Iteration 878, loss = 0.12495419\n",
            "Iteration 879, loss = 0.12490969\n",
            "Iteration 880, loss = 0.12483627\n",
            "Iteration 881, loss = 0.12479466\n",
            "Iteration 882, loss = 0.12476610\n",
            "Iteration 883, loss = 0.12471464\n",
            "Iteration 884, loss = 0.12462862\n",
            "Iteration 885, loss = 0.12461718\n",
            "Iteration 886, loss = 0.12463581\n",
            "Iteration 887, loss = 0.12455791\n",
            "Iteration 888, loss = 0.12462449\n",
            "Iteration 889, loss = 0.12445553\n",
            "Iteration 890, loss = 0.12442965\n",
            "Iteration 891, loss = 0.12435587\n",
            "Iteration 892, loss = 0.12431720\n",
            "Iteration 893, loss = 0.12426280\n",
            "Iteration 894, loss = 0.12424481\n",
            "Iteration 895, loss = 0.12421160\n",
            "Iteration 896, loss = 0.12419958\n",
            "Iteration 897, loss = 0.12411586\n",
            "Iteration 898, loss = 0.12409939\n",
            "Iteration 899, loss = 0.12403733\n",
            "Iteration 900, loss = 0.12404054\n",
            "Iteration 901, loss = 0.12407336\n",
            "Iteration 902, loss = 0.12396681\n",
            "Iteration 903, loss = 0.12388960\n",
            "Iteration 904, loss = 0.12388233\n",
            "Iteration 905, loss = 0.12381154\n",
            "Iteration 906, loss = 0.12378492\n",
            "Iteration 907, loss = 0.12373032\n",
            "Iteration 908, loss = 0.12373524\n",
            "Iteration 909, loss = 0.12364700\n",
            "Iteration 910, loss = 0.12358804\n",
            "Iteration 911, loss = 0.12357130\n",
            "Iteration 912, loss = 0.12351294\n",
            "Iteration 913, loss = 0.12354588\n",
            "Iteration 914, loss = 0.12343718\n",
            "Iteration 915, loss = 0.12337145\n",
            "Iteration 916, loss = 0.12339110\n",
            "Iteration 917, loss = 0.12341916\n",
            "Iteration 918, loss = 0.12330204\n",
            "Iteration 919, loss = 0.12326768\n",
            "Iteration 920, loss = 0.12323753\n",
            "Iteration 921, loss = 0.12323302\n",
            "Iteration 922, loss = 0.12319843\n",
            "Iteration 923, loss = 0.12311277\n",
            "Iteration 924, loss = 0.12313403\n",
            "Iteration 925, loss = 0.12304582\n",
            "Iteration 926, loss = 0.12324262\n",
            "Iteration 927, loss = 0.12290192\n",
            "Iteration 928, loss = 0.12289217\n",
            "Iteration 929, loss = 0.12290183\n",
            "Iteration 930, loss = 0.12289645\n",
            "Iteration 931, loss = 0.12283506\n",
            "Iteration 932, loss = 0.12278302\n",
            "Iteration 933, loss = 0.12283240\n",
            "Iteration 934, loss = 0.12275710\n",
            "Iteration 935, loss = 0.12270832\n",
            "Iteration 936, loss = 0.12282752\n",
            "Iteration 937, loss = 0.12275796\n",
            "Iteration 938, loss = 0.12252157\n",
            "Iteration 939, loss = 0.12263731\n",
            "Iteration 940, loss = 0.12263148\n",
            "Iteration 941, loss = 0.12254372\n",
            "Iteration 942, loss = 0.12244691\n",
            "Iteration 943, loss = 0.12244285\n",
            "Iteration 944, loss = 0.12239557\n",
            "Iteration 945, loss = 0.12235882\n",
            "Iteration 946, loss = 0.12239068\n",
            "Iteration 947, loss = 0.12234367\n",
            "Iteration 948, loss = 0.12230054\n",
            "Iteration 949, loss = 0.12219925\n",
            "Iteration 950, loss = 0.12224627\n",
            "Iteration 951, loss = 0.12209780\n",
            "Iteration 952, loss = 0.12228235\n",
            "Iteration 953, loss = 0.12209880\n",
            "Iteration 954, loss = 0.12209383\n",
            "Iteration 955, loss = 0.12203250\n",
            "Iteration 956, loss = 0.12205693\n",
            "Iteration 957, loss = 0.12193795\n",
            "Iteration 958, loss = 0.12196299\n",
            "Iteration 959, loss = 0.12192682\n",
            "Iteration 960, loss = 0.12193545\n",
            "Iteration 961, loss = 0.12188675\n",
            "Iteration 962, loss = 0.12183762\n",
            "Iteration 963, loss = 0.12188355\n",
            "Iteration 964, loss = 0.12176328\n",
            "Iteration 965, loss = 0.12175906\n",
            "Iteration 966, loss = 0.12170519\n",
            "Iteration 967, loss = 0.12165795\n",
            "Iteration 968, loss = 0.12168038\n",
            "Iteration 969, loss = 0.12162841\n",
            "Iteration 970, loss = 0.12156398\n",
            "Iteration 971, loss = 0.12157389\n",
            "Iteration 972, loss = 0.12152030\n",
            "Iteration 973, loss = 0.12145736\n",
            "Iteration 974, loss = 0.12140822\n",
            "Iteration 975, loss = 0.12148575\n",
            "Iteration 976, loss = 0.12143661\n",
            "Iteration 977, loss = 0.12135089\n",
            "Iteration 978, loss = 0.12136976\n",
            "Iteration 979, loss = 0.12133454\n",
            "Iteration 980, loss = 0.12124597\n",
            "Iteration 981, loss = 0.12135713\n",
            "Iteration 982, loss = 0.12134165\n",
            "Iteration 983, loss = 0.12126988\n",
            "Iteration 984, loss = 0.12122836\n",
            "Iteration 985, loss = 0.12116794\n",
            "Iteration 986, loss = 0.12109814\n",
            "Iteration 987, loss = 0.12107837\n",
            "Iteration 988, loss = 0.12104024\n",
            "Iteration 989, loss = 0.12099302\n",
            "Iteration 990, loss = 0.12105741\n",
            "Iteration 991, loss = 0.12099778\n",
            "Iteration 992, loss = 0.12096260\n",
            "Iteration 993, loss = 0.12091716\n",
            "Iteration 994, loss = 0.12088236\n",
            "Iteration 995, loss = 0.12087245\n",
            "Iteration 996, loss = 0.12089224\n",
            "Iteration 997, loss = 0.12093869\n",
            "Iteration 998, loss = 0.12086060\n",
            "Iteration 999, loss = 0.12071795\n",
            "Iteration 1000, loss = 0.12071349\n",
            "Iteration 1001, loss = 0.12072550\n",
            "Iteration 1002, loss = 0.12065976\n",
            "Iteration 1003, loss = 0.12077033\n",
            "Iteration 1004, loss = 0.12068990\n",
            "Iteration 1005, loss = 0.12071143\n",
            "Iteration 1006, loss = 0.12056645\n",
            "Iteration 1007, loss = 0.12053213\n",
            "Iteration 1008, loss = 0.12049264\n",
            "Iteration 1009, loss = 0.12050623\n",
            "Iteration 1010, loss = 0.12047368\n",
            "Iteration 1011, loss = 0.12045558\n",
            "Iteration 1012, loss = 0.12046460\n",
            "Iteration 1013, loss = 0.12048267\n",
            "Iteration 1014, loss = 0.12031642\n",
            "Iteration 1015, loss = 0.12036158\n",
            "Iteration 1016, loss = 0.12035410\n",
            "Iteration 1017, loss = 0.12029464\n",
            "Iteration 1018, loss = 0.12030937\n",
            "Iteration 1019, loss = 0.12023423\n",
            "Iteration 1020, loss = 0.12021135\n",
            "Iteration 1021, loss = 0.12021324\n",
            "Iteration 1022, loss = 0.12018064\n",
            "Iteration 1023, loss = 0.12014389\n",
            "Iteration 1024, loss = 0.12011612\n",
            "Iteration 1025, loss = 0.12009460\n",
            "Iteration 1026, loss = 0.12011827\n",
            "Iteration 1027, loss = 0.12000204\n",
            "Iteration 1028, loss = 0.12007700\n",
            "Iteration 1029, loss = 0.12003610\n",
            "Iteration 1030, loss = 0.11994893\n",
            "Iteration 1031, loss = 0.11995479\n",
            "Iteration 1032, loss = 0.11993899\n",
            "Iteration 1033, loss = 0.11990787\n",
            "Iteration 1034, loss = 0.11984887\n",
            "Iteration 1035, loss = 0.11983060\n",
            "Iteration 1036, loss = 0.11983569\n",
            "Iteration 1037, loss = 0.11977835\n",
            "Iteration 1038, loss = 0.11986703\n",
            "Iteration 1039, loss = 0.11977572\n",
            "Iteration 1040, loss = 0.11975725\n",
            "Iteration 1041, loss = 0.11971473\n",
            "Iteration 1042, loss = 0.11973039\n",
            "Iteration 1043, loss = 0.11966518\n",
            "Iteration 1044, loss = 0.11969837\n",
            "Iteration 1045, loss = 0.11966045\n",
            "Iteration 1046, loss = 0.11959008\n",
            "Iteration 1047, loss = 0.11960665\n",
            "Iteration 1048, loss = 0.11954137\n",
            "Iteration 1049, loss = 0.11959362\n",
            "Iteration 1050, loss = 0.11956560\n",
            "Iteration 1051, loss = 0.11950497\n",
            "Iteration 1052, loss = 0.11951895\n",
            "Iteration 1053, loss = 0.11953650\n",
            "Iteration 1054, loss = 0.11948524\n",
            "Iteration 1055, loss = 0.11946925\n",
            "Iteration 1056, loss = 0.11935575\n",
            "Iteration 1057, loss = 0.11931547\n",
            "Iteration 1058, loss = 0.11932475\n",
            "Iteration 1059, loss = 0.11937250\n",
            "Iteration 1060, loss = 0.11924808\n",
            "Iteration 1061, loss = 0.11933756\n",
            "Iteration 1062, loss = 0.11928575\n",
            "Iteration 1063, loss = 0.11921712\n",
            "Iteration 1064, loss = 0.11926785\n",
            "Iteration 1065, loss = 0.11922603\n",
            "Iteration 1066, loss = 0.11919971\n",
            "Iteration 1067, loss = 0.11917339\n",
            "Iteration 1068, loss = 0.11921187\n",
            "Iteration 1069, loss = 0.11917789\n",
            "Iteration 1070, loss = 0.11911707\n",
            "Iteration 1071, loss = 0.11913470\n",
            "Iteration 1072, loss = 0.11907026\n",
            "Iteration 1073, loss = 0.11900992\n",
            "Iteration 1074, loss = 0.11899564\n",
            "Iteration 1075, loss = 0.11899395\n",
            "Iteration 1076, loss = 0.11896628\n",
            "Iteration 1077, loss = 0.11894232\n",
            "Iteration 1078, loss = 0.11892684\n",
            "Iteration 1079, loss = 0.11890489\n",
            "Iteration 1080, loss = 0.11890053\n",
            "Iteration 1081, loss = 0.11885235\n",
            "Iteration 1082, loss = 0.11883750\n",
            "Iteration 1083, loss = 0.11884368\n",
            "Iteration 1084, loss = 0.11881127\n",
            "Iteration 1085, loss = 0.11880961\n",
            "Iteration 1086, loss = 0.11901343\n",
            "Iteration 1087, loss = 0.11880259\n",
            "Iteration 1088, loss = 0.11881034\n",
            "Iteration 1089, loss = 0.11870389\n",
            "Iteration 1090, loss = 0.11871892\n",
            "Iteration 1091, loss = 0.11873662\n",
            "Iteration 1092, loss = 0.11870204\n",
            "Iteration 1093, loss = 0.11870903\n",
            "Iteration 1094, loss = 0.11864401\n",
            "Iteration 1095, loss = 0.11859058\n",
            "Iteration 1096, loss = 0.11858630\n",
            "Iteration 1097, loss = 0.11857837\n",
            "Iteration 1098, loss = 0.11856672\n",
            "Iteration 1099, loss = 0.11860681\n",
            "Iteration 1100, loss = 0.11861051\n",
            "Iteration 1101, loss = 0.11859616\n",
            "Iteration 1102, loss = 0.11853298\n",
            "Iteration 1103, loss = 0.11846794\n",
            "Iteration 1104, loss = 0.11844132\n",
            "Iteration 1105, loss = 0.11846769\n",
            "Iteration 1106, loss = 0.11840297\n",
            "Iteration 1107, loss = 0.11841270\n",
            "Iteration 1108, loss = 0.11838189\n",
            "Iteration 1109, loss = 0.11832656\n",
            "Iteration 1110, loss = 0.11837486\n",
            "Iteration 1111, loss = 0.11838825\n",
            "Iteration 1112, loss = 0.11831058\n",
            "Iteration 1113, loss = 0.11839097\n",
            "Iteration 1114, loss = 0.11831562\n",
            "Iteration 1115, loss = 0.11825628\n",
            "Iteration 1116, loss = 0.11825138\n",
            "Iteration 1117, loss = 0.11823009\n",
            "Iteration 1118, loss = 0.11821316\n",
            "Iteration 1119, loss = 0.11822092\n",
            "Iteration 1120, loss = 0.11824581\n",
            "Iteration 1121, loss = 0.11823896\n",
            "Iteration 1122, loss = 0.11813463\n",
            "Iteration 1123, loss = 0.11818622\n",
            "Iteration 1124, loss = 0.11827005\n",
            "Iteration 1125, loss = 0.11812083\n",
            "Iteration 1126, loss = 0.11807776\n",
            "Iteration 1127, loss = 0.11819943\n",
            "Iteration 1128, loss = 0.11813347\n",
            "Iteration 1129, loss = 0.11805500\n",
            "Iteration 1130, loss = 0.11803404\n",
            "Iteration 1131, loss = 0.11800872\n",
            "Iteration 1132, loss = 0.11796278\n",
            "Iteration 1133, loss = 0.11793982\n",
            "Iteration 1134, loss = 0.11793750\n",
            "Iteration 1135, loss = 0.11800965\n",
            "Iteration 1136, loss = 0.11791429\n",
            "Iteration 1137, loss = 0.11793127\n",
            "Iteration 1138, loss = 0.11793161\n",
            "Iteration 1139, loss = 0.11789570\n",
            "Iteration 1140, loss = 0.11785461\n",
            "Iteration 1141, loss = 0.11783458\n",
            "Iteration 1142, loss = 0.11781800\n",
            "Iteration 1143, loss = 0.11787125\n",
            "Iteration 1144, loss = 0.11780895\n",
            "Iteration 1145, loss = 0.11776735\n",
            "Iteration 1146, loss = 0.11775019\n",
            "Iteration 1147, loss = 0.11771798\n",
            "Iteration 1148, loss = 0.11774142\n",
            "Iteration 1149, loss = 0.11770323\n",
            "Iteration 1150, loss = 0.11771541\n",
            "Iteration 1151, loss = 0.11771099\n",
            "Iteration 1152, loss = 0.11767933\n",
            "Iteration 1153, loss = 0.11764969\n",
            "Iteration 1154, loss = 0.11766069\n",
            "Iteration 1155, loss = 0.11766459\n",
            "Iteration 1156, loss = 0.11763462\n",
            "Iteration 1157, loss = 0.11758325\n",
            "Iteration 1158, loss = 0.11757540\n",
            "Iteration 1159, loss = 0.11757707\n",
            "Iteration 1160, loss = 0.11762902\n",
            "Iteration 1161, loss = 0.11755396\n",
            "Iteration 1162, loss = 0.11755587\n",
            "Iteration 1163, loss = 0.11750588\n",
            "Iteration 1164, loss = 0.11746677\n",
            "Iteration 1165, loss = 0.11753007\n",
            "Iteration 1166, loss = 0.11744302\n",
            "Iteration 1167, loss = 0.11749039\n",
            "Iteration 1168, loss = 0.11748871\n",
            "Iteration 1169, loss = 0.11753592\n",
            "Iteration 1170, loss = 0.11754290\n",
            "Iteration 1171, loss = 0.11736828\n",
            "Iteration 1172, loss = 0.11732034\n",
            "Iteration 1173, loss = 0.11732919\n",
            "Iteration 1174, loss = 0.11751307\n",
            "Iteration 1175, loss = 0.11734298\n",
            "Iteration 1176, loss = 0.11736438\n",
            "Iteration 1177, loss = 0.11732757\n",
            "Iteration 1178, loss = 0.11732462\n",
            "Iteration 1179, loss = 0.11736535\n",
            "Iteration 1180, loss = 0.11738321\n",
            "Iteration 1181, loss = 0.11723724\n",
            "Iteration 1182, loss = 0.11727195\n",
            "Iteration 1183, loss = 0.11725835\n",
            "Iteration 1184, loss = 0.11717737\n",
            "Iteration 1185, loss = 0.11724780\n",
            "Iteration 1186, loss = 0.11722221\n",
            "Iteration 1187, loss = 0.11721774\n",
            "Iteration 1188, loss = 0.11718763\n",
            "Iteration 1189, loss = 0.11720773\n",
            "Iteration 1190, loss = 0.11710532\n",
            "Iteration 1191, loss = 0.11714570\n",
            "Iteration 1192, loss = 0.11722241\n",
            "Iteration 1193, loss = 0.11713063\n",
            "Iteration 1194, loss = 0.11718547\n",
            "Iteration 1195, loss = 0.11713358\n",
            "Iteration 1196, loss = 0.11705675\n",
            "Iteration 1197, loss = 0.11704325\n",
            "Iteration 1198, loss = 0.11703534\n",
            "Iteration 1199, loss = 0.11700345\n",
            "Iteration 1200, loss = 0.11701006\n",
            "Iteration 1201, loss = 0.11709127\n",
            "Iteration 1202, loss = 0.11701781\n",
            "Iteration 1203, loss = 0.11697074\n",
            "Iteration 1204, loss = 0.11697377\n",
            "Iteration 1205, loss = 0.11696768\n",
            "Iteration 1206, loss = 0.11697540\n",
            "Iteration 1207, loss = 0.11700587\n",
            "Iteration 1208, loss = 0.11724111\n",
            "Iteration 1209, loss = 0.11702525\n",
            "Iteration 1210, loss = 0.11690403\n",
            "Iteration 1211, loss = 0.11690682\n",
            "Iteration 1212, loss = 0.11698181\n",
            "Iteration 1213, loss = 0.11693099\n",
            "Iteration 1214, loss = 0.11684559\n",
            "Iteration 1215, loss = 0.11696266\n",
            "Iteration 1216, loss = 0.11681239\n",
            "Iteration 1217, loss = 0.11685256\n",
            "Iteration 1218, loss = 0.11681255\n",
            "Iteration 1219, loss = 0.11685356\n",
            "Iteration 1220, loss = 0.11691540\n",
            "Iteration 1221, loss = 0.11677642\n",
            "Iteration 1222, loss = 0.11675102\n",
            "Iteration 1223, loss = 0.11678955\n",
            "Iteration 1224, loss = 0.11681443\n",
            "Iteration 1225, loss = 0.11668991\n",
            "Iteration 1226, loss = 0.11670942\n",
            "Iteration 1227, loss = 0.11670984\n",
            "Iteration 1228, loss = 0.11671580\n",
            "Iteration 1229, loss = 0.11667918\n",
            "Iteration 1230, loss = 0.11667029\n",
            "Iteration 1231, loss = 0.11664885\n",
            "Iteration 1232, loss = 0.11666760\n",
            "Iteration 1233, loss = 0.11659689\n",
            "Iteration 1234, loss = 0.11659600\n",
            "Iteration 1235, loss = 0.11660842\n",
            "Iteration 1236, loss = 0.11676224\n",
            "Iteration 1237, loss = 0.11672909\n",
            "Iteration 1238, loss = 0.11670334\n",
            "Iteration 1239, loss = 0.11660073\n",
            "Iteration 1240, loss = 0.11661386\n",
            "Iteration 1241, loss = 0.11659439\n",
            "Iteration 1242, loss = 0.11650729\n",
            "Iteration 1243, loss = 0.11655133\n",
            "Iteration 1244, loss = 0.11655297\n",
            "Iteration 1245, loss = 0.11653653\n",
            "Iteration 1246, loss = 0.11652424\n",
            "Iteration 1247, loss = 0.11649468\n",
            "Iteration 1248, loss = 0.11649803\n",
            "Iteration 1249, loss = 0.11645954\n",
            "Iteration 1250, loss = 0.11650072\n",
            "Iteration 1251, loss = 0.11648774\n",
            "Iteration 1252, loss = 0.11659683\n",
            "Iteration 1253, loss = 0.11646570\n",
            "Iteration 1254, loss = 0.11647848\n",
            "Iteration 1255, loss = 0.11638191\n",
            "Iteration 1256, loss = 0.11639629\n",
            "Iteration 1257, loss = 0.11653201\n",
            "Iteration 1258, loss = 0.11643555\n",
            "Iteration 1259, loss = 0.11639852\n",
            "Iteration 1260, loss = 0.11639734\n",
            "Iteration 1261, loss = 0.11635332\n",
            "Iteration 1262, loss = 0.11634771\n",
            "Iteration 1263, loss = 0.11640997\n",
            "Iteration 1264, loss = 0.11629223\n",
            "Iteration 1265, loss = 0.11635891\n",
            "Iteration 1266, loss = 0.11640010\n",
            "Iteration 1267, loss = 0.11631546\n",
            "Iteration 1268, loss = 0.11633489\n",
            "Iteration 1269, loss = 0.11629411\n",
            "Iteration 1270, loss = 0.11626745\n",
            "Iteration 1271, loss = 0.11634245\n",
            "Iteration 1272, loss = 0.11623021\n",
            "Iteration 1273, loss = 0.11622571\n",
            "Iteration 1274, loss = 0.11626943\n",
            "Iteration 1275, loss = 0.11627213\n",
            "Iteration 1276, loss = 0.11621511\n",
            "Iteration 1277, loss = 0.11619968\n",
            "Iteration 1278, loss = 0.11616658\n",
            "Iteration 1279, loss = 0.11620672\n",
            "Iteration 1280, loss = 0.11623170\n",
            "Iteration 1281, loss = 0.11622008\n",
            "Iteration 1282, loss = 0.11617321\n",
            "Iteration 1283, loss = 0.11612576\n",
            "Iteration 1284, loss = 0.11610460\n",
            "Iteration 1285, loss = 0.11628585\n",
            "Iteration 1286, loss = 0.11610320\n",
            "Iteration 1287, loss = 0.11612697\n",
            "Iteration 1288, loss = 0.11622910\n",
            "Iteration 1289, loss = 0.11607358\n",
            "Iteration 1290, loss = 0.11607582\n",
            "Iteration 1291, loss = 0.11612553\n",
            "Iteration 1292, loss = 0.11617296\n",
            "Iteration 1293, loss = 0.11604767\n",
            "Iteration 1294, loss = 0.11601489\n",
            "Iteration 1295, loss = 0.11605157\n",
            "Iteration 1296, loss = 0.11607694\n",
            "Iteration 1297, loss = 0.11606019\n",
            "Iteration 1298, loss = 0.11605197\n",
            "Iteration 1299, loss = 0.11602603\n",
            "Iteration 1300, loss = 0.11603817\n",
            "Iteration 1301, loss = 0.11606645\n",
            "Iteration 1302, loss = 0.11600780\n",
            "Iteration 1303, loss = 0.11606802\n",
            "Iteration 1304, loss = 0.11606009\n",
            "Iteration 1305, loss = 0.11595662\n",
            "Iteration 1306, loss = 0.11599951\n",
            "Iteration 1307, loss = 0.11594555\n",
            "Iteration 1308, loss = 0.11591484\n",
            "Iteration 1309, loss = 0.11598827\n",
            "Iteration 1310, loss = 0.11601737\n",
            "Iteration 1311, loss = 0.11592750\n",
            "Iteration 1312, loss = 0.11588600\n",
            "Iteration 1313, loss = 0.11591916\n",
            "Iteration 1314, loss = 0.11607919\n",
            "Iteration 1315, loss = 0.11591119\n",
            "Iteration 1316, loss = 0.11587828\n",
            "Iteration 1317, loss = 0.11583949\n",
            "Iteration 1318, loss = 0.11582061\n",
            "Iteration 1319, loss = 0.11593136\n",
            "Iteration 1320, loss = 0.11589867\n",
            "Iteration 1321, loss = 0.11592798\n",
            "Iteration 1322, loss = 0.11581644\n",
            "Iteration 1323, loss = 0.11577257\n",
            "Iteration 1324, loss = 0.11581751\n",
            "Iteration 1325, loss = 0.11578964\n",
            "Iteration 1326, loss = 0.11578665\n",
            "Iteration 1327, loss = 0.11575836\n",
            "Iteration 1328, loss = 0.11576530\n",
            "Iteration 1329, loss = 0.11576106\n",
            "Iteration 1330, loss = 0.11582492\n",
            "Iteration 1331, loss = 0.11581543\n",
            "Iteration 1332, loss = 0.11594781\n",
            "Iteration 1333, loss = 0.11575281\n",
            "Iteration 1334, loss = 0.11570622\n",
            "Iteration 1335, loss = 0.11574278\n",
            "Iteration 1336, loss = 0.11591878\n",
            "Iteration 1337, loss = 0.11577032\n",
            "Iteration 1338, loss = 0.11570288\n",
            "Iteration 1339, loss = 0.11565958\n",
            "Iteration 1340, loss = 0.11565358\n",
            "Iteration 1341, loss = 0.11569611\n",
            "Iteration 1342, loss = 0.11566729\n",
            "Iteration 1343, loss = 0.11564571\n",
            "Iteration 1344, loss = 0.11566017\n",
            "Iteration 1345, loss = 0.11566262\n",
            "Iteration 1346, loss = 0.11565304\n",
            "Iteration 1347, loss = 0.11564416\n",
            "Iteration 1348, loss = 0.11567954\n",
            "Iteration 1349, loss = 0.11561067\n",
            "Iteration 1350, loss = 0.11556993\n",
            "Iteration 1351, loss = 0.11560529\n",
            "Iteration 1352, loss = 0.11556607\n",
            "Iteration 1353, loss = 0.11554340\n",
            "Iteration 1354, loss = 0.11551922\n",
            "Iteration 1355, loss = 0.11558446\n",
            "Iteration 1356, loss = 0.11558457\n",
            "Iteration 1357, loss = 0.11561067\n",
            "Iteration 1358, loss = 0.11553937\n",
            "Iteration 1359, loss = 0.11552535\n",
            "Iteration 1360, loss = 0.11555277\n",
            "Iteration 1361, loss = 0.11553989\n",
            "Iteration 1362, loss = 0.11550459\n",
            "Iteration 1363, loss = 0.11551475\n",
            "Iteration 1364, loss = 0.11548363\n",
            "Iteration 1365, loss = 0.11561729\n",
            "Iteration 1366, loss = 0.11547858\n",
            "Iteration 1367, loss = 0.11546746\n",
            "Iteration 1368, loss = 0.11546193\n",
            "Iteration 1369, loss = 0.11550673\n",
            "Iteration 1370, loss = 0.11546813\n",
            "Iteration 1371, loss = 0.11553981\n",
            "Iteration 1372, loss = 0.11544555\n",
            "Iteration 1373, loss = 0.11541286\n",
            "Iteration 1374, loss = 0.11546803\n",
            "Iteration 1375, loss = 0.11543352\n",
            "Iteration 1376, loss = 0.11549819\n",
            "Iteration 1377, loss = 0.11546226\n",
            "Iteration 1378, loss = 0.11564948\n",
            "Iteration 1379, loss = 0.11549228\n",
            "Iteration 1380, loss = 0.11542659\n",
            "Iteration 1381, loss = 0.11537240\n",
            "Iteration 1382, loss = 0.11536033\n",
            "Iteration 1383, loss = 0.11538265\n",
            "Iteration 1384, loss = 0.11541135\n",
            "Iteration 1385, loss = 0.11533332\n",
            "Iteration 1386, loss = 0.11532272\n",
            "Iteration 1387, loss = 0.11535552\n",
            "Iteration 1388, loss = 0.11536509\n",
            "Iteration 1389, loss = 0.11529197\n",
            "Iteration 1390, loss = 0.11541731\n",
            "Iteration 1391, loss = 0.11536074\n",
            "Iteration 1392, loss = 0.11530680\n",
            "Iteration 1393, loss = 0.11536451\n",
            "Iteration 1394, loss = 0.11538008\n",
            "Iteration 1395, loss = 0.11526995\n",
            "Iteration 1396, loss = 0.11543293\n",
            "Iteration 1397, loss = 0.11544017\n",
            "Iteration 1398, loss = 0.11529204\n",
            "Iteration 1399, loss = 0.11532736\n",
            "Iteration 1400, loss = 0.11523689\n",
            "Iteration 1401, loss = 0.11532768\n",
            "Iteration 1402, loss = 0.11531212\n",
            "Iteration 1403, loss = 0.11532016\n",
            "Iteration 1404, loss = 0.11523823\n",
            "Iteration 1405, loss = 0.11522496\n",
            "Iteration 1406, loss = 0.11522905\n",
            "Iteration 1407, loss = 0.11522962\n",
            "Iteration 1408, loss = 0.11523017\n",
            "Iteration 1409, loss = 0.11520229\n",
            "Iteration 1410, loss = 0.11517623\n",
            "Iteration 1411, loss = 0.11524440\n",
            "Iteration 1412, loss = 0.11516333\n",
            "Iteration 1413, loss = 0.11522984\n",
            "Iteration 1414, loss = 0.11525115\n",
            "Iteration 1415, loss = 0.11518600\n",
            "Iteration 1416, loss = 0.11516793\n",
            "Iteration 1417, loss = 0.11511974\n",
            "Iteration 1418, loss = 0.11514952\n",
            "Iteration 1419, loss = 0.11511860\n",
            "Iteration 1420, loss = 0.11517606\n",
            "Iteration 1421, loss = 0.11511775\n",
            "Iteration 1422, loss = 0.11511570\n",
            "Iteration 1423, loss = 0.11509777\n",
            "Iteration 1424, loss = 0.11513180\n",
            "Iteration 1425, loss = 0.11510569\n",
            "Iteration 1426, loss = 0.11509440\n",
            "Iteration 1427, loss = 0.11515030\n",
            "Iteration 1428, loss = 0.11509255\n",
            "Iteration 1429, loss = 0.11507155\n",
            "Iteration 1430, loss = 0.11511954\n",
            "Iteration 1431, loss = 0.11507041\n",
            "Iteration 1432, loss = 0.11514879\n",
            "Iteration 1433, loss = 0.11540463\n",
            "Iteration 1434, loss = 0.11526532\n",
            "Iteration 1435, loss = 0.11516037\n",
            "Iteration 1436, loss = 0.11506516\n",
            "Iteration 1437, loss = 0.11504574\n",
            "Iteration 1438, loss = 0.11514034\n",
            "Iteration 1439, loss = 0.11506775\n",
            "Iteration 1440, loss = 0.11517461\n",
            "Iteration 1441, loss = 0.11499507\n",
            "Iteration 1442, loss = 0.11500006\n",
            "Iteration 1443, loss = 0.11499413\n",
            "Iteration 1444, loss = 0.11505008\n",
            "Iteration 1445, loss = 0.11507259\n",
            "Iteration 1446, loss = 0.11507689\n",
            "Iteration 1447, loss = 0.11503204\n",
            "Iteration 1448, loss = 0.11501846\n",
            "Iteration 1449, loss = 0.11496158\n",
            "Iteration 1450, loss = 0.11497829\n",
            "Iteration 1451, loss = 0.11501652\n",
            "Iteration 1452, loss = 0.11504935\n",
            "Iteration 1453, loss = 0.11516568\n",
            "Iteration 1454, loss = 0.11497255\n",
            "Iteration 1455, loss = 0.11498314\n",
            "Iteration 1456, loss = 0.11495522\n",
            "Iteration 1457, loss = 0.11493619\n",
            "Iteration 1458, loss = 0.11490917\n",
            "Iteration 1459, loss = 0.11498797\n",
            "Iteration 1460, loss = 0.11494311\n",
            "Iteration 1461, loss = 0.11496518\n",
            "Iteration 1462, loss = 0.11491913\n",
            "Iteration 1463, loss = 0.11490642\n",
            "Iteration 1464, loss = 0.11486944\n",
            "Iteration 1465, loss = 0.11490754\n",
            "Iteration 1466, loss = 0.11496094\n",
            "Iteration 1467, loss = 0.11486884\n",
            "Iteration 1468, loss = 0.11497337\n",
            "Iteration 1469, loss = 0.11495364\n",
            "Iteration 1470, loss = 0.11491658\n",
            "Iteration 1471, loss = 0.11488478\n",
            "Iteration 1472, loss = 0.11491373\n",
            "Iteration 1473, loss = 0.11490525\n",
            "Iteration 1474, loss = 0.11487597\n",
            "Iteration 1475, loss = 0.11478988\n",
            "Iteration 1476, loss = 0.11479817\n",
            "Iteration 1477, loss = 0.11482836\n",
            "Iteration 1478, loss = 0.11491335\n",
            "Iteration 1479, loss = 0.11482739\n",
            "Iteration 1480, loss = 0.11485309\n",
            "Iteration 1481, loss = 0.11477318\n",
            "Iteration 1482, loss = 0.11475743\n",
            "Iteration 1483, loss = 0.11478605\n",
            "Iteration 1484, loss = 0.11478791\n",
            "Iteration 1485, loss = 0.11481754\n",
            "Iteration 1486, loss = 0.11482527\n",
            "Iteration 1487, loss = 0.11478847\n",
            "Iteration 1488, loss = 0.11480244\n",
            "Iteration 1489, loss = 0.11475353\n",
            "Iteration 1490, loss = 0.11473157\n",
            "Iteration 1491, loss = 0.11481309\n",
            "Iteration 1492, loss = 0.11474683\n",
            "Iteration 1493, loss = 0.11472852\n",
            "Iteration 1494, loss = 0.11473837\n",
            "Iteration 1495, loss = 0.11479994\n",
            "Iteration 1496, loss = 0.11487619\n",
            "Iteration 1497, loss = 0.11471444\n",
            "Iteration 1498, loss = 0.11469511\n",
            "Iteration 1499, loss = 0.11468470\n",
            "Iteration 1500, loss = 0.11466107\n",
            "0.944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.944"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KFTCFFljlBwP",
        "outputId": "f373197f-29a2-4af5-da18-ae0320703d36"
      },
      "source": [
        "%reset -f\n",
        "import pickle\n",
        "with open('census.pkl', 'rb') as f:  \n",
        "  X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pickle.load(f)\n",
        "from sklearn.neural_network import MLPClassifier \n",
        "print(X_census_treinamento.shape) # Neur√¥nios = (108 + 1)/2 = 55\n",
        "rede_neural_census = MLPClassifier(verbose=True, max_iter = 10000, tol=0.000001,\n",
        "                                  hidden_layer_sizes = (55,55,55,55))\n",
        "rede_neural_census.fit(X_census_treinamento, y_census_treinamento)\n",
        "previsoes = rede_neural_census.predict(X_census_teste)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(accuracy_score(y_census_teste, previsoes))\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm = ConfusionMatrix(rede_neural_census)\n",
        "cm.fit(X_census_treinamento, y_census_treinamento)\n",
        "cm.score(X_census_teste, y_census_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27676, 108)\n",
            "Iteration 1, loss = 0.40015825\n",
            "Iteration 2, loss = 0.32700675\n",
            "Iteration 3, loss = 0.31384543\n",
            "Iteration 4, loss = 0.30598995\n",
            "Iteration 5, loss = 0.30069690\n",
            "Iteration 6, loss = 0.29647566\n",
            "Iteration 7, loss = 0.29312316\n",
            "Iteration 8, loss = 0.28910545\n",
            "Iteration 9, loss = 0.28603716\n",
            "Iteration 10, loss = 0.28270746\n",
            "Iteration 11, loss = 0.27899605\n",
            "Iteration 12, loss = 0.27718512\n",
            "Iteration 13, loss = 0.27264363\n",
            "Iteration 14, loss = 0.27235402\n",
            "Iteration 15, loss = 0.26792457\n",
            "Iteration 16, loss = 0.26348624\n",
            "Iteration 17, loss = 0.26116753\n",
            "Iteration 18, loss = 0.25886277\n",
            "Iteration 19, loss = 0.25635646\n",
            "Iteration 20, loss = 0.25470173\n",
            "Iteration 21, loss = 0.25110243\n",
            "Iteration 22, loss = 0.24816590\n",
            "Iteration 23, loss = 0.24449123\n",
            "Iteration 24, loss = 0.24166275\n",
            "Iteration 25, loss = 0.23893643\n",
            "Iteration 26, loss = 0.23529543\n",
            "Iteration 27, loss = 0.23234124\n",
            "Iteration 28, loss = 0.22859571\n",
            "Iteration 29, loss = 0.22795032\n",
            "Iteration 30, loss = 0.22423110\n",
            "Iteration 31, loss = 0.22340082\n",
            "Iteration 32, loss = 0.21748710\n",
            "Iteration 33, loss = 0.21705925\n",
            "Iteration 34, loss = 0.21350082\n",
            "Iteration 35, loss = 0.21184960\n",
            "Iteration 36, loss = 0.21003105\n",
            "Iteration 37, loss = 0.20725925\n",
            "Iteration 38, loss = 0.20660908\n",
            "Iteration 39, loss = 0.20254611\n",
            "Iteration 40, loss = 0.20095278\n",
            "Iteration 41, loss = 0.19797829\n",
            "Iteration 42, loss = 0.19593026\n",
            "Iteration 43, loss = 0.19793674\n",
            "Iteration 44, loss = 0.19517601\n",
            "Iteration 45, loss = 0.19273755\n",
            "Iteration 46, loss = 0.18751003\n",
            "Iteration 47, loss = 0.18726802\n",
            "Iteration 48, loss = 0.18484018\n",
            "Iteration 49, loss = 0.18445377\n",
            "Iteration 50, loss = 0.18231202\n",
            "Iteration 51, loss = 0.18045619\n",
            "Iteration 52, loss = 0.18480233\n",
            "Iteration 53, loss = 0.17979279\n",
            "Iteration 54, loss = 0.17877227\n",
            "Iteration 55, loss = 0.17873839\n",
            "Iteration 56, loss = 0.17501865\n",
            "Iteration 57, loss = 0.17194933\n",
            "Iteration 58, loss = 0.17008930\n",
            "Iteration 59, loss = 0.16950062\n",
            "Iteration 60, loss = 0.16930401\n",
            "Iteration 61, loss = 0.16771376\n",
            "Iteration 62, loss = 0.16733680\n",
            "Iteration 63, loss = 0.16536947\n",
            "Iteration 64, loss = 0.16493087\n",
            "Iteration 65, loss = 0.16298974\n",
            "Iteration 66, loss = 0.16183232\n",
            "Iteration 67, loss = 0.16383835\n",
            "Iteration 68, loss = 0.16005876\n",
            "Iteration 69, loss = 0.15754862\n",
            "Iteration 70, loss = 0.15842168\n",
            "Iteration 71, loss = 0.15822806\n",
            "Iteration 72, loss = 0.15631189\n",
            "Iteration 73, loss = 0.15662096\n",
            "Iteration 74, loss = 0.15285326\n",
            "Iteration 75, loss = 0.15319222\n",
            "Iteration 76, loss = 0.15495377\n",
            "Iteration 77, loss = 0.15383914\n",
            "Iteration 78, loss = 0.15790716\n",
            "Iteration 79, loss = 0.15124627\n",
            "Iteration 80, loss = 0.14976164\n",
            "Iteration 81, loss = 0.14769925\n",
            "Iteration 82, loss = 0.14650016\n",
            "Iteration 83, loss = 0.14901526\n",
            "Iteration 84, loss = 0.14693551\n",
            "Iteration 85, loss = 0.14558610\n",
            "Iteration 86, loss = 0.15384645\n",
            "Iteration 87, loss = 0.15254169\n",
            "Iteration 88, loss = 0.14413968\n",
            "Iteration 89, loss = 0.14383652\n",
            "Iteration 90, loss = 0.14135832\n",
            "Iteration 91, loss = 0.14135611\n",
            "Iteration 92, loss = 0.14605811\n",
            "Iteration 93, loss = 0.14011008\n",
            "Iteration 94, loss = 0.13957387\n",
            "Iteration 95, loss = 0.13582743\n",
            "Iteration 96, loss = 0.13713509\n",
            "Iteration 97, loss = 0.13570969\n",
            "Iteration 98, loss = 0.13776222\n",
            "Iteration 99, loss = 0.13341649\n",
            "Iteration 100, loss = 0.13709705\n",
            "Iteration 101, loss = 0.13462499\n",
            "Iteration 102, loss = 0.14136906\n",
            "Iteration 103, loss = 0.13651208\n",
            "Iteration 104, loss = 0.13834644\n",
            "Iteration 105, loss = 0.13351663\n",
            "Iteration 106, loss = 0.12906138\n",
            "Iteration 107, loss = 0.13215840\n",
            "Iteration 108, loss = 0.13353924\n",
            "Iteration 109, loss = 0.13405428\n",
            "Iteration 110, loss = 0.13588707\n",
            "Iteration 111, loss = 0.13352713\n",
            "Iteration 112, loss = 0.12909699\n",
            "Iteration 113, loss = 0.13068047\n",
            "Iteration 114, loss = 0.12565089\n",
            "Iteration 115, loss = 0.12791168\n",
            "Iteration 116, loss = 0.12937782\n",
            "Iteration 117, loss = 0.13019259\n",
            "Iteration 118, loss = 0.12663392\n",
            "Iteration 119, loss = 0.12691700\n",
            "Iteration 120, loss = 0.12726856\n",
            "Iteration 121, loss = 0.12571972\n",
            "Iteration 122, loss = 0.12332094\n",
            "Iteration 123, loss = 0.12309004\n",
            "Iteration 124, loss = 0.12528348\n",
            "Iteration 125, loss = 0.12507538\n",
            "Iteration 126, loss = 0.12378254\n",
            "Iteration 127, loss = 0.12393105\n",
            "Iteration 128, loss = 0.12003159\n",
            "Iteration 129, loss = 0.12170433\n",
            "Iteration 130, loss = 0.12199406\n",
            "Iteration 131, loss = 0.12834917\n",
            "Iteration 132, loss = 0.13018576\n",
            "Iteration 133, loss = 0.13042717\n",
            "Iteration 134, loss = 0.12283717\n",
            "Iteration 135, loss = 0.11950978\n",
            "Iteration 136, loss = 0.12095336\n",
            "Iteration 137, loss = 0.11922145\n",
            "Iteration 138, loss = 0.11618083\n",
            "Iteration 139, loss = 0.11848518\n",
            "Iteration 140, loss = 0.11988773\n",
            "Iteration 141, loss = 0.11886618\n",
            "Iteration 142, loss = 0.12035758\n",
            "Iteration 143, loss = 0.11698282\n",
            "Iteration 144, loss = 0.11629413\n",
            "Iteration 145, loss = 0.11265897\n",
            "Iteration 146, loss = 0.12195734\n",
            "Iteration 147, loss = 0.11797451\n",
            "Iteration 148, loss = 0.12186914\n",
            "Iteration 149, loss = 0.12645679\n",
            "Iteration 150, loss = 0.11805255\n",
            "Iteration 151, loss = 0.11698299\n",
            "Iteration 152, loss = 0.11580074\n",
            "Iteration 153, loss = 0.11270026\n",
            "Iteration 154, loss = 0.11175766\n",
            "Iteration 155, loss = 0.11063148\n",
            "Iteration 156, loss = 0.11582516\n",
            "Iteration 157, loss = 0.11523510\n",
            "Iteration 158, loss = 0.11311383\n",
            "Iteration 159, loss = 0.11288871\n",
            "Iteration 160, loss = 0.11532060\n",
            "Iteration 161, loss = 0.11171499\n",
            "Iteration 162, loss = 0.11162546\n",
            "Iteration 163, loss = 0.11286692\n",
            "Iteration 164, loss = 0.11006570\n",
            "Iteration 165, loss = 0.11322006\n",
            "Iteration 166, loss = 0.11071826\n",
            "Iteration 167, loss = 0.11444262\n",
            "Iteration 168, loss = 0.11214330\n",
            "Iteration 169, loss = 0.11424490\n",
            "Iteration 170, loss = 0.10858820\n",
            "Iteration 171, loss = 0.10913072\n",
            "Iteration 172, loss = 0.10460419\n",
            "Iteration 173, loss = 0.10673908\n",
            "Iteration 174, loss = 0.10778586\n",
            "Iteration 175, loss = 0.10813407\n",
            "Iteration 176, loss = 0.10820384\n",
            "Iteration 177, loss = 0.11167025\n",
            "Iteration 178, loss = 0.11423677\n",
            "Iteration 179, loss = 0.11243784\n",
            "Iteration 180, loss = 0.11342443\n",
            "Iteration 181, loss = 0.10747079\n",
            "Iteration 182, loss = 0.10240393\n",
            "Iteration 183, loss = 0.10213101\n",
            "Iteration 184, loss = 0.10006799\n",
            "Iteration 185, loss = 0.10468367\n",
            "Iteration 186, loss = 0.10268252\n",
            "Iteration 187, loss = 0.10200956\n",
            "Iteration 188, loss = 0.10110143\n",
            "Iteration 189, loss = 0.10511517\n",
            "Iteration 190, loss = 0.10397567\n",
            "Iteration 191, loss = 0.10631655\n",
            "Iteration 192, loss = 0.10756280\n",
            "Iteration 193, loss = 0.10429607\n",
            "Iteration 194, loss = 0.10398061\n",
            "Iteration 195, loss = 0.10379573\n",
            "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
            "0.8128966223132037\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8128966223132037"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEPCAYAAABCyrPIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATSElEQVR4nO3deZhVdf3A8fcMA4OCijCKgCao+MnlR+aghmBqmWWumWUppWhpaYtKmuaGS6Lh8jM3MolUKJRSU4PckFTQ1FH0h8TXSikVBJSlQmQZ5vfHfBkvWyYOHpn7fj2Pz3PmnHOvn/t45T1nuZeKhoYGJEmqLHoASdKHg0GQJAEGQZKUGQRJEmAQJElZVdEDrK26urpqYDdgBlBf8DiStL5oBXQBnqqtrV1UumG9DQKNMXi06CEkaT21F/BY6Yr1OQgzACYcP4i3Z80pehZpBd9/eRzM+23RY0irWLzhwbz44ouQ/wwttT4HoR7g7VlzWDjjjaJnkVZQXV0NrZcUPYa0qjZtli+tcqrdi8qSJMAgSJIygyBJAgyCJCkzCJIkwCBIkjKDIEkCDIIkKTMIkiTAIEiSMoMgSQIMgiQpMwiSJMAgSJIygyBJAgyCJCkzCJIkwCBIkjKDIEkCDIIkKTMIkiTAIEiSMoMgSQIMgiQpMwiSJMAgSJIygyBJAgyCJCkzCJIkwCBIkjKDIEkCDIIkKTMIkiTAIEiSMoMgSQIMgiQpMwiSJMAgSJIygyBJAgyCJCkzCJIkwCBIkjKDIEkCDIIkKTMIkiTAIEiSMoMgSQKgqugBtG5VbdCWw355Ke06d6KqbTWPXHQ9M5+byqHDB1PZuoplS5ZyR//TWTDzDTr3Cg4ZdgkA6XcP8cjF1wPQZ+Bx9Op/CPVLljLmpAuY/vT/FfmS1IItXLiYnfudzbkDD+HTe+/IgO8MY8nSelpXtWLE0BN4bcY8Bp7366b9p6Tp3HXr99hz954FTt1yrLMgRMR4oB2wIK8amFKqi4jTgS8BDcAFKaUxEXEssHNK6Qf5sVcC9Sml09fVfOUiDt6X6U9PZuKQm9jkI1352gO/4JXHJ1F34+1MGT2W3U46ij6nDeDBHw7hoBsv4p4TzuX1SX/m8JGXU7VBWzbtsSU7f+VAbuz9RTr3Cj566KcNgtaZi6+4m44d2gNwzo/v4IRj9uHLh+3OdTc9yJU33MdPBh3J+LvPAmDe/AUc2v+nfKL3tkWO3KKsdRAiog8wJaU0/z/sNiClNLnkMT2ArwB9gE2ARyPivpWedwCwDXD42s6md7xw+9im5Y236sI/X53JmJMuYOnbiwBYMHsuXXbdiXabd6JN+w15/dkpANxx1EAAtj9oX164fSwN9fW8/uyUpu1Sc5v64nSmpOkcuH8vAK4f8nXatm0NwGY1G/PM839fYf/Lr/0Dp5y4P5WVnvluLu85CBFxAHAa8BpwckSMBjYr2WVxSmn/NTx8X2BsSmkxMDsi/g7sWPLcewInAPullJa919m0ZsdN+DUbb7kFvzroWyx5ayEAFZWV7HbyUTxy4XV06N6NhXPmc+jwwXTs2Z0po//An66+mQ7du7Gsvp6jx95EZesq7j9tMDOfTwW/GrVEA88bxbWXfY2bRz0GQLt21QDU1y/jumEPcd4PDm3ad+HCxdw3bjIXnvWFQmZtqf7rIETEIcCPgKeB41NK/8ibvvQfHnZhRNQAfwZOAbYAZpdsnwV0ycsfAe4EDkspLUDN6hd9v0rnj32Uw0cMYejHDqGispIv3PoTpo17gpfHPUG3PT7Gpj225LbDTmbJwrc5/vHbeOmBCVBRQWWrVow84Bts1beWg2/6MTftfkTRL0ctzC2jJtCn93b02HqzFdbX1y/ja9++kU/ttQOf3rvpd0fuGvMMB+7fy6ODZvZejhC+C9wDDMm/4b+bq4HnU0p/i4gbgJNXs09FyfIewKXAkIjYJ6W09D3MpjXosutOLJj1Jv989XVmPjeVyqpWbLhZR/a//IfM+cvf+eOF1wGwYOabzHrhLyycMw+AVx6rY7OderJg5hu8MfWlxnUT6ujQvVthr0Ut1+8feI6Xps3m3vsn8er0uVRXV7Fl147cctsEem7TmfPPOGyF/e+9fxLfHvCpgqZtud5LEI4ATgImRsRI4MaU0oI1nTJKKd1Zsu4e4EjgYSBK1ncDpgNdgdEppasiYlvgAuDs9/5ytLKtP9mbTbbuxn2nXtJ0nWDbz/SlfvESxg+6pmm/edNepXqjdrTddBPenvdPOu+yA3U33sb8f0yn97e+wuRRv6dTbMM/X5lR4KtRS3XbsJOalgdddifdt6ph5uz5tGlTxQVnrnpa6KlnX2bo5Vt9kCOWhf86CPni8eB8B9CxwIMRcWhKaZVTRhFRATwAHJFSmgfsA0wGxgGnRcT5QA2NQZgC9C55+OnAkxExLqX00Fq9KjV5eugoDhn2Y459ZCStN2jLmJMvpN9ZJ1DVtppjHr4FgNlT/saYky/gvlMHc/TYn0NDA3/9w6NN1wp6HvBJjps4CoAxJ19Y2GtReblu2DjeXrSEfQ4ZDMCO23fj+su/DsC8+W+x0UYbFDlei1TR0NCwVg+MiEqgck2ndiLiy8APabzt9DUarzu8FRHfBY6m8bbTc1JKD63mttOPAXcBe6SUZq3u+evq6roDLz908PdYOOONtXoN0rpyfkOCOTcXPYa0ikXtvsLkyZMBetTW1k4r3bbWt53mu4DWeCdQSul24PbVrL8GuGaldb9c6efngB5rO5sk6b3zEr0kCTAIkqTMIEiSAIMgScoMgiQJMAiSpMwgSJIAgyBJygyCJAkwCJKkzCBIkgCDIEnKDIIkCTAIkqTMIEiSAIMgScoMgiQJMAiSpMwgSJIAgyBJygyCJAkwCJKkzCBIkgCDIEnKDIIkCTAIkqTMIEiSAIMgScoMgiQJMAiSpMwgSJIAgyBJygyCJAkwCJKkzCBIkgCDIEnKDIIkCTAIkqTMIEiSAIMgScoMgiQJMAiSpMwgSJIAgyBJygyCJAkwCJKkzCBIkgCDIEnKqooe4P0avskcZr49u+gxpBWcD9DxmKLHkFa1aNEaN633QZg0/lyqWy8pegxpBR07dmTOX68qegxpFbv0vZQRI0asdpunjCRJgEGQJGUGQZIEGARJUmYQJEmAQZAkZQZBkgQYBElSZhAkSYBBkCRlBkGSBBgESVJmECRJgEGQJGUGQZIEGARJUmYQJEmAQZAkZQZBkgQYBElSZhAkSYBBkCRlBkGSBBgESVJmECRJgEGQJGUGQZIEGARJUmYQJEmAQZAkZQZBkgQYBElSZhAkSYBBkCRlBkGSBBgESVJmECRJgEGQJGUGQZIEGARJUmYQJEmAQZAkZQZBkgQYBElSZhAkSYBBkCRlBkGSBBgESVJWVfQA+mAtXLiYnfudzbkDD2Gb7pvzo4t/Q+vWrWi3YTW33nACE5/8C0OuHdu0/zPP/Z2pTwyma5dNC5xaLd2wEX/k1tsnNv389KRpTBhzNt/+wS1UVECvHbfihiuOYdo/ZvM//c6ldpetAdis00aMHv6dosZucdZZECLil0At8GZeNSSl9PuIOBo4BVgG3JhSGhYR+wDfSSkdkR/7PaAfcGRKqWFdzViOLr7ibjp2aA/Aaef8mpE/O5Ho2YVLrryHn/3yYc485SAO3H8XAP760kx+cN4oY6B17vj+e3N8/70B+OOEqdx+15OccvavuPqSo9ht12046oShjH3weXbYvgux3RaMv/usgidumd53ECJiX2BiSmnRajaflVK6t2TfdsB5wO7AYuCpiLhzpef7DHAksJ8xaF5TX5zOlDSdA/fvBUBNp/a8OfffAMydv4DYrssK+w+67C7OP+OwD3xOlbcLh/yO4dd+g08edAm77boNAAd/dhce/OML7LB9l3d5tN6P5riG0B2YEBFnRMTG77LvHsBTKaX5KaWFwASg7/KNEdETGAJ8MW9XMxp43iiuvPirTT9fdfFRHPa1nxK7n8mjj7/IsV/t17Rt+oy5vD5rPh/vtXURo6pMPfXMS2zVrSNVrSrZtEO7pvWb12zMjJnzAXh91nyOOPZa9vzcxYwcPXFNT6W18L6DkFIaTuMf6vOBhyPikohomzd/JyLGRcSoiKgBtgBmlzx8FrA8+ZsAdwOXpZRef79zaUW3jJpAn97b0WPrzZrWfffMEdx5y/dIT15Kv09sz/XDxjVtu3nUBPp/qU8Ro6qM3TTikRV+MVmuoaHxZEGnTdtz0VmH8+uff4u7R36fcwffyYzX533QY7ZYzXINIZ8u+llETAZGAzcAtwJvppQmRcSZwCBg5ZxXlCzXAucA50TEvSmlfzXHbGr0+wee46Vps7n3/km8On0u1dVVzJ33Fn336AnAZ/bZiZGjH19h/1E//3ZR46pMjX9sKtdc2p+KCnhzzr+b1r82Yy5dt+jARhttwICj9wKgptNG9N6lO1P/MoMuW3QoauQWpVmCEBH7AacDbwCfSym9ArxSssvdNEbiNzQeJSzXDXgiL49LKV0fERvmffs3x2xqdNuwk5qWB112J923quHKG+5jytTX2PGj3XjqmZfpuW3npn1emjabLbt1LGJUlanpM+bSvn01bdo0/rH00Z5deOyJF+n3ie254946vvvN/Xj40T9zz32TuPLir7JgwSImTf4H22+7xbs8s/5bzXFReSiwBDgxpTStZP1vgdNTSi8B+wCTgT8BN0VEB2ApjaeaTgE+XvKUVwAPRMSAfDpK68jQK47hm6cOp3XrVnTs0J5fXHMc0Pib2SYbb1DwdCo3M2bOZ/Oady5D/u+Pj+LEgTezbNky9qjdlv322YmlS+u5edRj9PnsRdTXL+Os7x9Et67eBddcKpafm1tbEdEmpbR4Nev3BX4CvAX8GxiQUpoVEUfQeDTRAFyTUhq5mttOuwKPA59NKU1d3b+3rq6uO/DyzlsmqlsveV+vQWpuHbc7lTl/varoMaRV7ND3UkaMGAHQo7a2dlrptvd9hLC6GOT1DwO7rWb9b2g8dVS6bjwwvuTn6YC3t0jSB8ivrpAkAQZBkpQZBEkSYBAkSZlBkCQBBkGSlBkESRJgECRJmUGQJAEGQZKUGQRJEmAQJEmZQZAkAQZBkpQZBEkSYBAkSZlBkCQBBkGSlBkESRJgECRJmUGQJAEGQZKUGQRJEmAQJEmZQZAkAQZBkpQZBEkSYBAkSZlBkCQBBkGSlBkESRJgECRJmUGQJAEGQZKUGQRJEmAQJEmZQZAkAQZBkpQZBEkSYBAkSZlBkCQBBkGSlBkESRJgECRJmUGQJAEGQZKUGQRJEmAQJElZVdEDvA+tABYvXZ9fglqqzp07s2hJ66LHkFZRU1OzfLHVytsqGhoaPthpmkldXV0/4NGi55Ck9dRetbW1j5WuWJ9/vX4K2AuYAdQXPIskrS9aAV1o/DN0BevtEYIkqXl5UVmSBBgESVJmECRJgEGQJGUGQZIEGASViIiad99LKkZEbF/0DC2dQSgzEXHZGtZ/CvjTBzyOtIKIGBURbVaz/ljgvg9+ovKyPn8wTWunKiLuAI5OKS2MiErgQuBg4AvFjiYxEXgoIg5PKc2OiHbAUKAzsGexo7V8fjCtDEXEAOAbwGnA5cCzwBkppbcLHUwCIuIzwGX5n3OB4SmlK4qdqjwYhDIVEX2AO4BLU0pXFz2PVCoitgN+B1yTUhpa9DzlwiCUmYgYAiz/j741sDswevn2lNIZRcwlAUTEaN55f3YAdgXGLd+eUvpyEXOVC68hlJ/JJcsvAGOKGkRajWuLHqCceYRQhiKiAtiRxm88BJieUppS4EhSk4joAPSl5P0JPJpS+ldxU5UHg1BmIuIA4EpgGjAbqAC6AV2Bb6WUxhc2nMpeRBwHnAo8xorvzz2BQSmlUQWO1+J5yqj8nAfslVJ6o3RlRHSl8VpC30Kmkhp9E9ht5TveIqI9cD9gENYhP5hWfiqBuatZPwvfDypeK1b/i2olvj/XOY8Qys9vgCciYiyNh+TQeK7288DPC5tKanQ18HREPMmK78/ewJmFTVUmvIZQhiKiO7AvsEVe9RrwcErplcKGkrKI2BDYgxXfn0/6wcl1zyCUmYj4eErp2bzcFjgZ2JnG21Gv8386FSkiDkgpjc3LnYBBvPP+vGDla19qXp6TKz+lXwFwFbANcDuwOTCskImkd5xesnwN8ApwEjAFGF7IRGXEawjlp6JkeceU0t55eWxEjC9gHmlNOqeUfpKX/xwRfkp5HTMI5WfDiNiBxjDMjogeKaWXI2IToH3Bs0k1EfH5vLwoInqllJ6PiB5AuyIHKwcGofy8BVxf8nMv4GUa7z66tJCJpHfUAV/KyzOBTnl5COD3bK1jXlSW9KEUEdVATUrptaJnKRdeVC5TEfGH1S1LHyLnAX+KiJ2LHqRcGITyVb2GZalwEbER8Fngy8APCx6nbBiE8tWwhmXpw+BE4MaU0kRgs4j4SNEDlQODIOlDJSLaAEfyzucOhgADi5uofBiE8vXGGpalou1J46fmlwCklB4C2kSEd0WuY95lVIby+dn6lNJbJetqgDkppWXFTSapSB4hlKedKPmrCvOXif3OGEjlzSCUoZTSE8CWEbFlXvVN4OYCR5L0IWAQyteVwBn5vGx//OIwqex5DaGMRcTjwN1AQ0rJr62QypxHCOVtMI1/Uc51RQ8iqXgeIUiSAI8QJEmZQZAkAQZBkpQZBEkSYBAkSZlBkCQBBkGSlBkESRIA/w+ynSoECm3TcQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}